# Class 5: Clustering Documents With Unsupervised Machine Learning

## Section 1: Use Metrics To Determine Text Similarity
### key concepts
| Lesson          |         Discription                                | Colab link    |
|-------------------|----------------------------------------------|------|
| **Module Intro**   | - become familiar with two major similarity measures: lexical and semantic similarity practice evaluating the similarity between words by computing the distance virus identification and spell-check problems using the Hamming and Levenshtein distance metrics    Measuring the similarity between words in this module will prepare you for the document categorization     |  |
|**Vid: pare Texts Using Similarity Metrics**|lexical similarity, which is based on syntax, structure, and content, is commonly used for autocomplete and spell check applications. Semantic similarity, by contrast, uses context to evaluate the similarity in meaning between words or documents.|
|**Compare Texts Using Similarity Metrics**|1 how to calculate **Jaccard similarity** and use a correlation function to compare two words. 2 practice applying Jaccard similarity to both characters and words.|
|**Code: Practice Comparing Texts Using Similarity Metrics**|Binary comparison 2 Jaccard similarity 3. Correlation 4 using ord() to convert to ASCII |[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matsunagateitoku/stdp-class/blob/main/4.1.1.Practice_Comparing_Texts.ipynb) |
|**Find Mismatches With Hamming Distance**|**Hamming distance** is another way to measure the distance between words, when working with two strings of equal length number of substitutions needed to make two strings equal|
|**Practice Finding Mismatches With Hamming Distance**|Hamming Distance=opposite of similarity. sequences of equal length, counts positions that corresponding characters are differen | [![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matsunagateitoku/stdp-class/blob/main/Practice_Finding_Mismatches.ipynb)|
|**Similarity Measures**|Quiz|
|**Comparing Sequences With Levenshtein Distance**|Levenshtein distance, counts the number of alterations needed to convert one string into another string using addition, deletion, or substitution| video
|**Comparing Sequences With Levenshtein Distance**|Levenshtein distance, counts the number of alterations needed to convert one string into another string using addition, deletion, or substitution| video
|**Compute Levenshtein Distance**| text explaintion of Levenshtein|
|**Practice Comparing Sequences With Levenshtein Distance**|compute Levenshtein distance using dynamic programming and how to model the results in a 2D matrix|[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matsunagateitoku/stdp-class/blob/main/class_5/Practice_Comparing_Sequences.ipynb)|
|**Use Levenshtein Distance To Autocorrect Text**|built-in Python-Levenshtein package is very fast bsed on C & NLTK implementation of edit distance  timeit function in conjunction with the Brown Corpus dictionary to offer recommendations for reducing the total number of computations and runtime|
|**Practice Using Levenshtein Distance To Autocorrect Text**||[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matsunagateitoku/stdp-class/blob/main/class_5/Practice_Using_Levenshtein.ipynb)|
|**Review: Similarity and Distances Metrics**||[PDF](class_5/cis575_tool-similarity-and-distance-metrics.pdf).
|**Course Project, Part One — Using Metrics To Determine Text Similarity**|1. computes Jaccard similarity 2. ompute Jaccard similarity scores for SsQry set and each presidential speech in NLTK 3. Hamming distance;; Marker(), which takes two same-length strings, query sQry and target sTgt, and computes returns a "marked" string sTgt where the characters matching to corresponding characters in sQry are replaced with _. 4 ankHD(), which takes two strings: query sQry and target sTgt, of varying lengths. Then, for each substring in sTgt with length equal to len(sQry), compute the Hamming distance and the corresponding marked string.|[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matsunagateitoku/stdp-class/blob/main/class_5/UsingMetrics.ipynb)|
|**Module Wrap-up: Use Metrics To Determine Text Similarity**|In this module, you gained a familiarity with several approaches and metrics used to measure the distance between words and documents. You began by comparing two words using Jaccard similarity, and then applied the Levenshtein and Hamming distance metrics to more complex problems. Finally, you built functions computing Jaccard similarity and Hamming distance, which can be used to evaluate the similarity between speeches and rank viral samples. |

## Section 2: Use Metrics To Determine Text Similarity

### Key Concepts:
-  Sentence Bidirectional Encoder Representations from Transformers (SBERT)
- Dendrogram

| Lesson          |         Discription                                | Colab link    |
|-------------------|----------------------------------------------|------|
|**Module Introduction**|Perform Hierarchical Clustering on Sentence Embeddings To Group Similar Texts|In this module, you will be introduced to **clustering**, an unsupervised method you will use to categorize documents based on their similarities. However, before you can practice clustering techniques in code, you will first examine how to **represent documents as numerical vectors**. Then, you will practice using **hierarchical clustering** to categorize movies in a dataset according to genre. Finally, you will use **dendrograms, or tree diagrams**, to visually evaluate the quality of your clustering. You will also evaluate your clustering quality using several quantitative
|**Embed Sentences Into Vectors**|When representing and evaluating many sentences in code, one helpful tool is Sentence Bidirectional Encoder Representations from Transformers (SBERT), which encodes entire sentences or documents as numerical vectors. SBERT can be used for a variety of NLP tasks including language translation and meaningful sentence embeddings.uses the sentence-transformers implementation of SBERT to encode 15 famous quotes. He then evaluates the quotes’ correlation coefficients and cosine similarities using a pre-trained model.|video
|**Practice Embedding Sentences Into Vectors**|Sentence BERT or SBERT incredibly easy to use and is only a few hundred megabytes compared to 8 gigabyte fastText model. Sentence BERT does not store static word vectors. Instead each word vector, if still needed, is generated dynamically from its semantic context. 1. SBERT in code--Sentence Transformers Package. 2. Encode function 3 loading a dictionary with famous quotes 4. Encode function to create 15 different 768-dimensional vectors.5) compute the correlation  [minus 1 and 1]. Next we can query the list of sentences that we have based on their vector representation. And this does not need to have the same-- we do not need to have the same words as we've had previously with TFIDF type of query or document term matrix type of query. Here the semantic meaning is extracted from the sentence we have, language and sight being the query sentence. And that is represented in the next line with a vector, again, 768 dimensional, which we use to compute every one of these cosine similarities for every one sentence.paraphrase-distilroberta-base-v (1330 MB) model. In this activity, you will use a smaller model, paraphrase-albert-small-v2 (~50 MB).  SBERT.encode().  evaluate correlations (i.e., pairwise measures of linear dependence), which can be easily computed using the .corr() method on the transposed dataframe.  **cosine similarity**| [![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matsunagateitoku/stdp-class/blob/main/class_5/Practice_Comparing_Sequences.ipynb)|
|**Compute Similarity of Sentence Embeddings To Find Similar Movies**|1) Practice using SBERT on a data frame of movies and their attributes. 2}Use a pre-trained model to evaluate cosine similarity of movies based on their descriptions and attributes. 3)Demonstrate how to display movie results based on a specific query.|video
|**Practice Computing Similarity of Sentence Embeddings To Find Similar Movies**|working with JSON   1) use a pre-trained model to evaluate the cosine similarity of a dataset of movies based on their descriptions 2) practice using SBERT and computing the cosine similarity on a movie dataset for different queries.use a smaller model, paraphrase-albert-small-v2 (~50 MB), not paraphrase-distilroberta-base-v1(330 MB) model. |[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matsunagateitoku/stdp-class/blob/main/class_5/Find_Movies.ipynb)|
|**Methods for Clustering**|use clustering, an unsupervised method for unlabeled documents, to group documents according to their similarities. the clustering process entails and introduces two standard clustering methods: **hierarchical clustering (HC) and k-means clustering**.| video
|**Hierarchical Clustering of Movies Based on Their Descriptions**|outlines two algorithms used to build hierarchical clustering trees, or dendrograms: the agglomerative approach and the divisive approach.  We cannot plot 768-dimensional vectors but we can compress that into two-dimensional space using PCA, Principal Component Analysis, which uses Singular Value Decomposition or SVD underneath. We do need to specify how many components we want to draw and the most important components are retrieved, which will be our dimensions. They are X and Y or PC1 and Principal Component 2|video
|**Practice Hierarchical Clustering of Movies Based on Their Descriptions**| demonstrated two algorithms for building hierarchical clustering trees and then used agglomerative clustering to classify movies from a database into two categories using embeddings and distance calculations. If you want to plot the movie vectors on a 2D plane, then you need to convert each 768D vector into a 2D representation. Principal component analysis (PCA) is a popular choice. It uses singular value decomposition (SVD) as its engine to find a new set of 768 axes along the most-explanatory (i.e., most variable) directions of the given vectors. Then two top axes (or coordinates) can be plotted and other 766 coordinates are dropped as least explanatory (of the underlying distribution pattern).|[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matsunagateitoku/stdp-class/blob/main/class_5/Hierarchical_Clustering.ipynb)|
|**The Anatomy of a Dendrogram**|Grouping points with a hierarchical clustering algorithm can produce a dendrogram, or tree diagram. decision trees, which model classification possibilities for labeled data in supervised learning. Dendrograms are similar but pertain to unlabeled data in unsupervised learning.  using the PlotDendrogram() function |
|**Interpreting a Dendrogram**|| quiz
|**Build Dendrograms Using Different Linkages**|Since a cluster is not a single point but spread out over space, there is no natural inter-cluster distance. In order to find the distance between clusters, you can use different inter-cluster metrics, or linkages, to find the distance between two points, A and B, in different clusters. Each linkage method has a different approach for selecting points A and B. describes several linkage methods and discusses some pros and cons of each. He then shows the resulting dendrograms in code, using the movie data to model different linkage methods.| video
|**Practice Building Dendrograms Using Different Linkages**|Previously, Professor Melnikov demonstrated how to create a dendrogram using the results of hierarchical clustering to evaluate the quality of the clustering and how to use different linkage methods to find the distance between clusters. In the "Review" section of this ungraded coding exercise, you will use these techniques as Professor Melnikov presented them in the video. In the "Optional Practice" section of this exercise, you will practice clustering movie vectors and how to determine the better linkage method by viewing a dendrogram.|[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matsunagateitoku/stdp-class/blob/main/class_5/Dendrograms.ipynb)|
|**Identify Linkages in Dendrograms**||quiz
|**Measure Clustering Performance**|demonstrates how to compute several quantitative metrics in code. In this example, he uses agglomerative clustering to group a set of movie vectors into two clusters. He then identifies the accuracy score, Rand index or adjusted Rand index (ARI), and silhouette score as three metrics to describe the clustering performance.| video
|**Practice Measuring Clustering Performance**|Previously, Professor Melnikov demonstrated how to compute a variety of quantitative metrics to measure cluster performance from agglomerative clustering and showed how results vary with a different number of clusters. In the "Review" section of this ungraded coding exercise, you will use these techniques as Professor Melnikov presented them in the video. In the "Optional Practice" section of this exercise, you will practice writing a function to compute accuracy scores to measure cluster performance and determine the ideal number of clusters.|need to study [![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matsunagateitoku/stdp-class/blob/main/class_5/Measuring_Clustering.ipynb)
|**Course Project, Part Two — Performing Hierarchical Clustering on Sentence Embeddings To Group Similar Texts**|encoding movie descriptions into numeric vectors in order to cluster them with various sets of parameters and determining the best set of parameters for hierarchical clustering using the silhouette score. |[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matsunagateitoku/stdp-class/blob/main/class_5/Performing_Hierarchical.ipynb)|
|**Module Wrap-up**|Perform Hierarchical Clustering on Sentence Embeddings To Group Similar Texts|practiced converting documents to numeric vectors using SBERT,  applied hierarchical clustering to a dataset of movies, categorizing movies into two clusters according to genre. evaluated the performance of your clustering using dendrograms and three quantitative metrics: the accuracy score, the Rand index or adjusted Rand index (ARI), and the silhouette score. At the beginning of this module, you were introduced to two main clustering approaches. Now that you’ve practiced using hierarchical clustering to categorize your documents, you will move on to another approach: k-means clustering. 

## Section 3: Perform K-Means Clustering on Sentence Embeddings To Group Similar Texts
| Lesson          |         Discription                                | Colab link    |
|-------------------|----------------------------------------------|------|
|**Module Introduction: Perform K-Means Clustering on Sentence Embeddings To Group Similar Texts**||explored hierarchical clustering and practiced embedding sentences as vectors using SBERT. use SBERT to represent texts as vectors. introduced to k-means clustering, which you will practice throughout the module. After using k-means clustering to categorize movies, you will evaluate the quality of your clusters using silhouette scores. You will also encounter another type of problem in this module: identifying an item within a set that is representative of the set as a whole.
|**Calculate Centroids and Medoids To Find a Representative Data Point**|centroid is the mean vector of a set of vectors and is used to represent a set of vectors.  When you apply this concept to a corpus and you want to find a representative document in the corpus, finding a centroid will not work as it is the mean and does not correspond to one of the documents. A medoid, the closest existing vector to the centroid, allows you to find a representative document, which can be used to help determine what labels should be applied to the cluster. Watch as Professor Melnikov defines these terms and plots examples that compare the centroid and medoid for different sets of vectors.| video
|**Practice Calculating Centroids and Medoids To Find a Representative Data Point**|pretty straight forward | code 1 [![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matsunagateitoku/stdp-class/blob/main/class_5/Calculating-Centroids.ipynb)
|**Compute the Medoid To Find a Representative Movie**|You can use the medoid to find a representative movie from the movie database for a specific genre. In this video, Professor Melnikov demonstrates this technique for action movies in code. Follow along as Professor Melnikov loads the movie database, uses JSON to parse and clean up fields, and filters out only the action movies. He then selects a single representative movie by identifying the medoid for a matrix containing vector representations of all action movies in the database. To visualize the results, he uses Principal Component Analysis (PCA) to compress the vectors from 768 dimensions to two-dimensional vectors and then plots the results. |Libraries/Methods Standard Library SentenceTransformer() JSON, SBERT.encode()  pd.dataframe, np.mean() np.argmin(), PCA() Plotly Library scatter(), layout() figure()  VIDEO
|**Practice Computing the Medoid To Find a Representative Movie**|| code 2 [![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matsunagateitoku/stdp-class/blob/main/class_5/Medoid_Movie.ipynb)
|**Clustering Movies With K-Means and Evaluating Performance**||
|**Practice Clustering Movies With K-Means and Evaluating Performance**|| code 3
|**Hierarchical and K-Means Clustering**|In this course, you have practiced two approaches for clustering movies into genres: hierarchical clustering and k-means clustering. Prior to clustering, you have preprocessed your data by embedding sentences into vectors with SBERT. After clustering, you have modeled your findings visually through plotting. This tool demonstrates the clustering process for both algorithms, from preprocessing to plotting. Use it as a guide whenever grouping data with hierarchical clustering or k-means clustering.| [text](class_5/cis575_tool-hierarchical-and-k-means-clustering.pdf)
|**Course Project, Part Three — Perform K-Means Clustering on Sentence Embeddings To Group Similar Texts**||
|**Module Wrap-up: Perform K-Means Clustering on Sentence Embeddings To Group Similar Texts**|In this module, you continued exploring clustering approaches, but also examined a new problem: how to identify a document that is representative of an entire corpus. First, you gained a familiarity with centroids and medoids — two vectors used to identify the average item in a set. You then used these vectors on a data set of movies, to practice identifying a representative action movie. Finally, you examined another approach for clustering documents: k-means clustering. Using your familiarity with SBERT from the previous module, you created sentence embeddings to categorized with this new clustering technique, and then used silhouette scores to evaluate the quality of your clustering.|
|**Glossary**||  [text](class_5/cis575_glossary.pdf)
|**Course Transcript**|| [text](./class_5/CIS575_course_transcript.pdf)

