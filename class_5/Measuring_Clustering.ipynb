{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup**\n",
    " \n",
    "Reset the Python environment to clear it of any previously loaded variables, functions, or libraries. Then, import the libraries needed to complete the code Professor Melnikov presented in the video.\n",
    "\n",
    "For brevity shorter alias names are used for the two metrics: `ARI` for [`adjusted_rand_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html) and `SIL` for [`silhouette_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html). Also `HC` alias is used for [`AgglomerativeClustering`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from IPython.core.interactiveshell import InteractiveShell as IS\n",
    "IS.ast_node_interactivity = \"all\"    # allows multiple outputs from a cell\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt \n",
    "from sklearn.cluster import AgglomerativeClustering as HC  # hierarchical clustering\n",
    "from sklearn.metrics import adjusted_rand_score as ARI, silhouette_score as SIL\n",
    "from sentence_transformers import SentenceTransformer  # encodes text documents to 768D vectors\n",
    "pd.set_option('max_rows', 5, 'max_columns', 40, 'max_colwidth', 100, 'precision', 2) # print format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">\n",
    "\n",
    "# **Review**\n",
    "\n",
    "Review the code Professor Melnikov used to subsample movies from The Movie Database ([TMDB](https://www.themoviedb.org/)) file `movies.zip`, encode their text attribute, cluster these movie vectors into two groups and compute several metrics of performance of this clustering mechanism.\n",
    "\n",
    "Changes from the video:\n",
    "\n",
    "1. As in the previous Jupyter Notebook (JN), the movies are filtered before they are encoded to avoid unnecessary encoding of unused movie descriptions\n",
    "1. A smaller (in file size) pre-trained [SBERT](https://www.sbert.net/) model is used\n",
    "1. Movie filtering is done simpler - based on language in the movie\n",
    "1. For simplicity, a single textual attribute is encoded\n",
    "1. Other minor code improvements are introduced\n",
    "\n",
    "## **Read Movie Attributes**\n",
    "\n",
    "The next cell reads the movie file, replaces missing (i.e., [NA](https://pandas.pydata.org/docs/user_guide/missing_data.html#missing-data-na)) values, sets movie titles as a row indices for each movie, and filters out all but two mutually exclusive languages, Spanish (with 32 movies) and German (with 27 movies). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movies.zip').fillna('').set_index('original_title') # load TMDB database\n",
    "df.original_language.value_counts().to_frame().T\n",
    "df = df.query('original_language==\"es\" or original_language==\"de\"')   # disjoint languages \n",
    "print('df.shape = ', df.shape)\n",
    "df[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The movie descriptions are encoded into a 768-dimensional space, where each movie is represented by a 768D numeric vector. Now mathematical calculations of distances (used in clustering) can be applied to any pair of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBERT = SentenceTransformer('paraphrase-albert-small-v2')  # load a pre-trained language model\n",
    "mEmb  = SBERT.encode(df.overview)  # embedding ~5K descriptions may take 10+ minutes\n",
    "pd.DataFrame(mEmb, index=df.title) # wrap matrix as dataframe with movie titles as indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a hierarchical clustering model is initialized and fitted on an embedding matrix of selected movies, `mEmb`, containing 59 rows as movie vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = HC(n_clusters=2).fit(mEmb)   # initialize and fit a clustering object on movie vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `hc.labels_` contains predicted cluster labels, which are always whole numbers starting from 0. These are assigned arbitrarily. So, one cluster can be assigned a label 0 for one movie sample and a label 1 for even a slightly different movie sample. This instability in label representation presents a problem if we want to compare estimated clusters to possibly known classes (if true labels are given in advance). \n",
    "\n",
    "Below several clustering performance measures are presented for situations when true labels are known and not known. Remember that clustering is an unsupervised learning method and does not use labels in identifying \"similar\" observations.\n",
    "\n",
    "First, `tY` is defined as a vector of labels, 0 for German, 1 for Spanish. This assignment is arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = hc.labels_   # predicted labels, i.e. numbers 0,1 for estimated clusters\n",
    "tY = (df.original_language=='es')*1  # true (observed) labels, 0=German, 1=Spanish\n",
    "pd.DataFrame({'predicted labels, pY':pY, 'true labels, tY': tY}).T # compare labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Accuracy Score**\n",
    "\n",
    "Accuracy, a number in the $[0,1]$ range, is computed for approximately balanced (i.e. equisized) clusters. It is simply a fraction of matched labels. Since label assignment is arbitrary, you can compute accuracy for `pY == tY` and for `pY != tY`, and just use the largest of the two. If you have more than two clusters, you will need to try all combinations of matched estimated and true labels before you choose the greatest computed accuracy as the actual measure of performance. The complementary fraction becomes the error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Equal_rate = sum(pY == tY) / len(tY)\n",
    "Unequal_rate = sum(pY != tY) / len(tY) # same as 1 - Equal_rate\n",
    "print(f'Accuracy for pY == tY : {max(Equal_rate, Unequal_rate):.3f}')   # largest ratio is the accuracy rate\n",
    "print(f'Error rate: {min(Equal_rate, Unequal_rate):.3f}') # Smallest ratio is the error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of 0.78 (highest of the two) is hard to judge unless you are well familiar with this domain. Still the clustering appears effective (and not random), especially, since are are essentially identifying the language of each movie based on the vector representation of its English-written *overview* text. If changes are made to the model, it becomes easier to judge the change in accuracy (and not its absolute value). Higher accuracy tends to be more desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Adjusted Rand Index**\n",
    "\n",
    "The [Rand index](https://scikit-learn.org/stable/modules/clustering.html#rand-index) (RI), developed by statistician William Rand, counts all *pairs of points assigned to the same clusters* and those assigned otherwise. RI uses the true labels, which are not always available, but a human expert can also evaluate a smaller sample of paired points.\n",
    "\n",
    "The [adjusted Rand index (ARI)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html#sklearn-metrics-adjusted-rand-score) rescales RI to be in [0,1] interval, where 0 indicates no pattern in clustering, and 1 is a perfect clustering with dense and well-separated groups. ARI is computed as\n",
    "\n",
    "$$\\frac{(\\text{RI} - \\text{RI}_{\\text{Expected}})}{ (\\max(\\text{RI}) - \\text{RI}_\\text{Expected})} \\in [0,1]$$\n",
    "\n",
    "Next we compute ARI by providing true and estimated labels. While the metric of 0.30 appears rather low (compared to the value 1), the relative comparisons of this metric are more meaningful. Your goal should be to improve 0.30 value, which may be exponentially harder as it approaches the value 1. Typically, once you develop an expertise with a particular corpus, you will be able to assess whether 0.30 is too low or whether it's better than one might expect with the given corpus. The improvement in ARI is what you are after. Once it becomes too hard or expensive to improve (towards 1), perhaps, it's a good time to move to another task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI(tY, pY)   # ARI âˆˆ [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Silhouette Score (Coefficient)**\n",
    "\n",
    "[Silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score) relies on inter- and intra- cluster distances without observation labels. It is computed as \n",
    "\n",
    "$$(b - a) / \\max(a, b) \\in [-1,1]$$\n",
    "\n",
    "where $a:=$ mean intra-cluster distance, $b:=$ mean nearest-cluster distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIL(mEmb, labels=pY)  # Silhouette Score, (b - a) / max(a, b) âˆˆ [-1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Use Metrics To Estimate Number of Clusters**\n",
    "\n",
    "Performance metrics are especially helpful when compared on a relative basis: you can choose a \"better\" number of clusters by selecting one with the best performance. For that, you need to try different numbers of clusters, $k$, say from two to 10 (and possibly larger for larger and more diverse sets). \n",
    "\n",
    "Below this is done with a function `ScoreClusters()`, which fits a hierarchical clustering model for the given $k$ and computes two metrics, ARI and silhouette score. Each $k$ and its corresponding measures are saved into a dataframe, which is plotted (metric vs $k$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScoreClusters(k=2, tY=[0,1,1], mEmb=mEmb)->(float, float):\n",
    "    '''Inputs:\n",
    "        k: number of clusters\n",
    "        tY: array/list of test/true labels\n",
    "        mEmb: embedding matrix with rows as sentence vectors'''\n",
    "    hc = HC(n_clusters=k).fit(mEmb)  # run clustering algorithm\n",
    "    return ARI(np.array(tY), hc.labels_), SIL(mEmb, labels=hc.labels_)\n",
    "\n",
    "K = range(2, 10)  # range of numbers of clusters\n",
    "dfSC = pd.DataFrame([ScoreClusters(k, pY, mEmb) for k in K], index=K, columns=['ARI','SIL'])\n",
    "dfSC.index.name='Number of clusters, k'\n",
    "ax = dfSC.plot(grid=True, figsize=[15,4], secondary_y='SIL', title='Clustering Performance @k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both metrics suggest that two is the best number of clusters (shows highest ARI and silhouette scores at $k=2$). This makes sense, since we picked movies that mainly differ in language attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">\n",
    "\n",
    "# **Optional Practice**\n",
    "\n",
    "Now you will practice writing a function to compute accuracy scores to measure cluster performance and determine the ideal number of clusters.\n",
    "\n",
    "As you work through these tasks, check your answers by running your code in the *#check solution here* cell, to see if youâ€™ve gotten the correct result. If you get stuck on a task, click the **See solution** drop-down to view the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 1**\n",
    "\n",
    "Write a UDF `Acc(pY, tY)->float` which takes two arrays (binary predicted labels and binary true labels) and computes accuracy as the best accuracy from each combination of true label assignments (as discussed above).\n",
    "\n",
    "<b>Hint:</b> You need to try every possible assignment of true labels, <code>tY</code>, and compute the corresponding accuracies. Then return the maximum. For the binary labels there are only two permutations of true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#606366>\n",
    "    <details><summary><font color=#B31B1B>â–¶ </font>See <b>solution</b>.</summary>\n",
    "<pre class=\"ec\">\n",
    "def Acc(pY=np.array([0,1,1]), tY=[0,0,1]) -> float:\n",
    "    return max(sum(pY == tY), sum(pY != tY)) / len(tY)\n",
    "Acc(pY, tY)\n",
    "</pre>\n",
    "</details> \n",
    "</font>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 2**\n",
    "\n",
    "Write a UDF `ScoreClusters2(k=2, tY, mEmb)->(float, float, float)`, which is similar to `ScoreClusters()`, but returns the accuracy score as well. Then run this UDF for $k\\in\\{2,3,...,30\\}$. You can print the results on screen; plotting is not necessary.\n",
    "\n",
    "What $k$ values of ARI, silhouette score, and accuracy imply the \"best\" clustering? Which one $k$ value would you choose using accuracy?\n",
    "\n",
    "<b>Hint:</b> You can add the output of <code>Acc()</code> UDF as an additional value of the tuple returned by <code>ScoreClusters2()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#606366>\n",
    "    <details><summary><font color=#B31B1B>â–¶ </font>See <b>solution</b>.</summary>\n",
    "<pre class=\"ec\">\n",
    "def ScoreClusters2(k=2, tY=[0,1,1], mEmb=mEmb)->(float, float, float):\n",
    "    '''Inputs:\n",
    "        k: number of clusters\n",
    "        tY: array/list of test/true labels\n",
    "        mEmb: embedding matrix with rows as sentence vectors'''\n",
    "    hc = HC(n_clusters=k).fit(mEmb)  # run clustering algorithm\n",
    "    return ARI(np.array(tY), hc.labels_), SIL(mEmb, labels=hc.labels_), Acc(hc.labels_, tY)\n",
    "\n",
    "K = range(2, 30)  # range of numbers of clusters\n",
    "dfSC2 = pd.DataFrame([ScoreClusters2(k, pY, mEmb) for k in K], index=K, columns=['ARI','SIL','ACC'])\n",
    "\n",
    "#- print k maximizers (i.e. k values yielding highest values of the metrics)\n",
    "print([list(dfSC2[m][dfSC2[m]==dfSC2[m].max()].index) for m in ['ARI','SIL','ACC']])\n",
    "\n",
    "#- plot performance metrics at each value of k\n",
    "ax = dfSC2.plot(grid=True, figsize=[15,4], secondary_y='SIL');\n",
    "plt.title('Clustering Performance at each k');\n",
    "ax.set_xlabel('number of clusters, k');\n",
    "            </pre>ARI suggests $k=2$, SIL suggests $k=21$, accuracy suggests $k\\in\\{2, 3, 6, 8, 11, 16, 18, 23\\}$. Typically, a \"simpler\" (or lower complexity) model should be preferred (for many reasons). Thus, given multiple $k$, we prefer the fewest number of clusters. Thus, $k=2$ is preferred using accuracy metric.\n",
    "</details>\n",
    "</font>\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
