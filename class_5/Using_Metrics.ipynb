{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part One of the Course Project**\n",
    "In this project, you will develop and apply Jaccard similarity to find similar presidential speeches and apply Hamming distance to find similar viral DNA sequences.\n",
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "adfd83c6c2e656fe2b8d4dc515968dfd",
     "grade": false,
     "grade_id": "cell-a9d40ca3a9daac71",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# **Setup**\n",
    " \n",
    "Reset the Python environment to clear it of any previously loaded variables, functions, or libraries. Then, import the libraries and corpora needed for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10e3cdd974e45178cf8ea6e3ef81e61b",
     "grade": false,
     "grade_id": "cell-afc1076a9a2b7f5a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = 'all'\n",
    "import nltk, string, pandas as pd, numpy as np, unittest, numpy.testing as npt\n",
    "from colorunittest import run_unittest\n",
    "_ = nltk.download(['inaugural', 'stopwords'], quiet=True) # silently load corpora from NLTK\n",
    "\n",
    "np.set_printoptions(linewidth=10000, precision=4, edgeitems=20, suppress=True) \n",
    "pd.set_option('max_colwidth', 200, 'display.max_rows', 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fd1aa2ddb0f425c4753c1f510fc2ca9",
     "grade": false,
     "grade_id": "cell-cfc888b2403494cb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Next, load two presidential speeches and the English list of common stop words from the NLTK corpus. `SsPunct` is a list of English punctuation symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eec4669d2349510809d5db337a0359cc",
     "grade": false,
     "grade_id": "cell-fc0ff824cf9b6687",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'fellow', 'citizens', ':', 'I', 'stand', 'here', 'today', 'humbled', 'by', 'the', 'task', 'before', 'us', ',']\n"
     ]
    }
   ],
   "source": [
    "LsObama9    = nltk.corpus.inaugural.words('2009-Obama.txt') # Text of Pres Obama's inaugural speech of 2009\n",
    "LsObama13   = nltk.corpus.inaugural.words('2013-Obama.txt') # Text of Pres Obama's inaugural speech of 2013\n",
    "SsStopwords = set(nltk.corpus.stopwords.words('english'))   # list of common English stop words \n",
    "SsPunct     = set(string.punctuation)                       # English punctuation symbols\n",
    "print(LsObama9[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c83bf202cedfb25a5a9d47794cc46d9e",
     "grade": false,
     "grade_id": "cell-a251ff33ed472b88",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## **Task 1**\n",
    "\n",
    "Complete the user defined function (UDF) `JS()`, which takes two sets of string tokens, `A` and `B`, and computes Jaccard similarity between them. The other parameters determine the required preprocessing of these sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60f217a3ad549f3eb268c40e91b781f9",
     "grade": false,
     "grade_id": "JS_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# COMPLETE THIS CELL\n",
    "def JS(A=set('ABC'), B=set('ABCD'), Lower=False, Stop=SsStopwords, Punct=SsPunct):\n",
    "    '''Jaccard similarity between sets of string tokens A & B. Evaluate arguments are in order listed.\n",
    "    If denominator is zero, return zero\n",
    "    Inputs:\n",
    "        A, B: two containers (set, list, etc.) with string elements.\n",
    "        Lower: indicates whether lower case should be used before removal of stopwords and punctuation\n",
    "        Stop: container of stopword strings\n",
    "        Punct: container of punctuation strings\n",
    "    Return: Jaccard similarity after preprocessing of A and B\n",
    "    '''\n",
    "    a, b = set(A), set(B)  # initialize a, b (cast to sets, just in case)\n",
    "#     if Lower: a, b = \n",
    "#     if Stop:  a, b = \n",
    "#     if Punct: a, b = \n",
    "    js = None  # final Jaccard similarity score\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version to work on\n",
    "\n",
    "def JS(A=set('ABC'), B=set('ABCD'), Lower=False, Stop=SsStopwords, Punct=SsPunct):\n",
    "    '''Jaccard similarity between sets of string tokens A & B. Evaluate arguments are in order listed.\n",
    "    If denominator is zero, return zero\n",
    "    Inputs:\n",
    "        A, B: two containers (set, list, etc.) with string elements.\n",
    "        Lower: indicates whether lower case should be used before removal of stopwords and punctuation\n",
    "        Stop: container of stopword strings\n",
    "        Punct: container of punctuation strings\n",
    "    Return: Jaccard similarity after preprocessing of A and B\n",
    "    '''\n",
    "    a, b = set(A), set(B)  # initialize a, b (cast to sets, just in case)\n",
    "#     if Lower: a, b = \n",
    "#     if Stop:  a, b = \n",
    "#     if Punct: a, b = \n",
    "    js = None  # final Jaccard similarity score\n",
    "\n",
    "    a, b = set(A), set(B) #make sets\n",
    "    \n",
    "    if Lower: # check lower \n",
    "        a = {x.lower() for x in a}\n",
    "        b = {x.lower() for x in b}\n",
    "    \n",
    "    if Stop:\n",
    "        a = {x for x in a if x not in Stop}\n",
    "        b = {x for x in b if x not in Stop}\n",
    "        \n",
    "    if Punct:\n",
    "        a = {x for x in a if x not in Punct}\n",
    "        b = {x for x in b if x not in Punct}\n",
    "    \n",
    "    #JaccardSim = lambda A={}, B={}: len(A & B) / len(A | B)    # A, B = sets of characters\n",
    "    \n",
    "    intersection = len(a.intersection(b))\n",
    "    union = len(a.union(b))\n",
    "    \n",
    "    if union == 0:\n",
    "        js = 0.0\n",
    "    else:\n",
    "        js = intersection / union\n",
    "    \n",
    "    return js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JS(\"cadcdythggbgbfb fb  bcdt\", \"cxcdcxcstt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c83061b1c6f0791caaf02bb58bbc958",
     "grade": false,
     "grade_id": "cell-57587265327e4cb8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The tests in the cell below can help you troubleshoot specific failing scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e2916790a79743d760e0890efc6c9cf",
     "grade": true,
     "grade_id": "JS_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 24 tests in 0.082s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test00 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test01 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test02 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test03 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test04 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test05 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test06 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test07 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test08 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test09 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test10 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test11 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test12 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test13 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test14 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test15 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test16 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test17 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test18 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test19 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test20 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test21 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test22 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test23 (__main__.Test_JS) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "ae = npt.assert_equal\n",
    "@run_unittest\n",
    "class Test_JS(unittest.TestCase): # class with methods to test functionality of the JS() function\n",
    "    def test00(self): ae(JS('', 'ABC'), 0)\n",
    "    def test01(self): ae(JS('ABC', ''), 0)\n",
    "    def test02(self): ae(JS('', ''), 0)\n",
    "    def test03(self): ae(JS('ABC', 'ABC'), 1)\n",
    "    def test04(self): ae(JS('ABC', 'ABc'), 0.5)\n",
    "    def test05(self): ae(JS('ABC', 'ABC', Stop='A'), 1)\n",
    "    def test06(self): ae(JS('ABC', 'ABc', Stop='c'), 0.6666666666666666)\n",
    "    def test07(self): ae(JS('ABC', 'ABc', Stop='Cc'), 1)\n",
    "    def test08(self): ae(JS('ABC!', 'ABC', Stop='A'), 1)\n",
    "    def test09(self): ae(JS('ABC!', 'ABc,', Stop='c'), 0.6666666666666666)\n",
    "    def test10(self): ae(JS('ABC!', 'ABc%', Stop='Cc'), 1)\n",
    "    def test11(self): ae(JS('ABC', 'ABC', Lower=True, Stop='A'), 1)\n",
    "    def test12(self): ae(JS('I like NLP'.split(), 'I like nlp'.split()), 0.5)\n",
    "    def test13(self): ae(JS('I like NLP'.split(), 'I like nlp'.split(), Lower=True, Stop={}), 1)\n",
    "    def test14(self): ae(JS('I like NLP'.split(), 'I like nlp'.split(), Lower=True, Stop=['nlp']), 1)\n",
    "    def test15(self): ae(JS('I like NLP'.split(), 'I like nlp'.split(), Lower=False, Stop=['NLP', 'nlp']), 1)\n",
    "    def test16(self): ae(JS(LsObama9, LsObama13, Lower=True, Stop={}, Punct={}),  0.25333333333333335)\n",
    "    def test17(self): ae(JS(LsObama9, LsObama13, Lower=False, Stop={}, Punct={}), 0.24857954545454544)\n",
    "    def test18(self): ae(JS(LsObama9, LsObama13, Lower=True, Punct={}),  0.21112006446414183)\n",
    "    def test19(self): ae(JS(LsObama9, LsObama13, Lower=False, Punct={}), 0.21137586471944658)\n",
    "    def test20(self): ae(JS(LsObama9, LsObama13, Lower=True, Stop={}), 0.2503725782414307)\n",
    "    def test21(self): ae(JS(LsObama9, LsObama13, Lower=False, Stop={}), 0.24571428571428572)\n",
    "    def test22(self): ae(JS(LsObama9, LsObama13, Lower=True), 0.20762368207623683)\n",
    "    def test23(self): ae(JS(LsObama9, LsObama13, Lower=False), 0.20804331013147717)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d8bd8bac76d7f3248f247b8cf52fb3c",
     "grade": false,
     "grade_id": "cell-de583cccd76ef4d8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## **Task 2**\n",
    "\n",
    "Complete UDF `JS_Speech()` and apply UDF `JS()` to compute Jaccard similarity scores for `SsQry` set and each presidential speech in NLTK (i.e. `nltk.corpus.inaugural.fileids()`). The JS score and `fid` of each speech is packaged as a row in Pandas DataFrame object (with a column `JS`). The `fid` value (without `.txt`) is used as the row index. Return results ordered by decreasing `JS`. So, the top speeches are most similar to the `SsQry` with respect to Jaccard similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ad6c7853161c9389b6eacd364699252",
     "grade": false,
     "grade_id": "JS_Speech_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def JS_Speech(SsQry=LsObama9, Lower=True, Stop=SsStopwords, Punct=SsPunct):\n",
    "    '''Compute Jaccard similarity of each presidential inaugural speech with SsQry set of words.\n",
    "    Inputs:\n",
    "      SsQry: a vocabulary, i.e. set of words describing the query document\n",
    "      Lower: indicates whether lower case should be used before removal of stopwords and punctuation\n",
    "      Stop: container of stopword strings\n",
    "      Punct: container of punctuation strings\n",
    "    Return: Dataframe with file id (fid) as row index (labeled as `fid`) and Jaccard score column (labeled `JS`)\n",
    "      Omit `.txt` from fid index values; i.e. use fid index '1993-Clinton' instead of original fid '1993-Clinton.txt'\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2005-Bush.txt', '2009-Obama.txt', '2013-Obama.txt', '2017-Trump.txt', '2021-Biden.txt']\n"
     ]
    }
   ],
   "source": [
    "FIDs = nltk.corpus.inaugural.fileids()[:59]  # load file IDs (incl. 2021-Biden). This list grows over years\n",
    "print(FIDs[-5:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vice President Cheney, Mr. Chief Justice, President Carter, President Bush, President Clinton, membe...',\n",
       " 'My fellow citizens:\\n\\nI stand here today humbled by the task before us, grateful for the trust you ha...',\n",
       " 'Thank you. Thank you so much.\\n\\nVice President Biden, Mr. Chief Justice, Members of the United States...',\n",
       " 'Chief Justice Roberts, President Carter, President Clinton, President Bush, President Obama, fellow ...',\n",
       " 'Chief Justice Roberts, Vice President Harris, Speaker Pelosi, Leader Schumer, Leader McConnell, Vice...']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LsDocs = [nltk.corpus.inaugural.raw(fid) for fid in FIDs]\n",
    "[s[:100]+'...' for s in LsDocs[-5:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-10-0b83fcc55326>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-0b83fcc55326>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    return df\u001b[0m\n\u001b[0m             \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"#version to work on\n",
    "\n",
    "def JS_Speech(SsQry=LsObama9, Lower=True, Stop=SsStopwords, Punct=SsPunct):\n",
    "    results = []\n",
    "\n",
    "    for fid, speech_text in FIDs():\n",
    "        words = set(speech_text.split())\n",
    "        jaccard_score = JS(SsQry, words, Lower, Stop, Punct)\n",
    "        results.append((fid.replace('.txt', ''), jaccard_score))\n",
    "    df = pd.DataFrame(results, columns=['JS'], index=pd.Series([fid for fid, _ in results]))\n",
    "    df.index.name = 'fid'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def JS_Speech(SsQry=LsObama9, Lower=True, Stop=SsStopwords, Punct=SsPunct):\n",
    "    FIDs = nltk.corpus.inaugural.fileids()\n",
    "    results = []\n",
    "\n",
    "    for fid in FIDs:\n",
    "        speech_text   = nltk.corpus.inaugural.words(fid)\n",
    "        jaccard_score = JS(SsQry, speech_text)\n",
    "        results.append((fid.replace('.txt', ''), jaccard_score))\n",
    "    \n",
    "    df = pd.DataFrame(results, columns=['fid', 'JS']) #, index=pd.Series([fid for fid, _ in results]))\n",
    "    df.set_index('fid', inplace=True)\n",
    "    df = df.sort_values(by='JS', ascending=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def JS_Speech(SsQry=LsObama9, Lower=True, Stop=SsStopwords, Punct=SsPunct):\n",
    "    FIDs = nltk.corpus.inaugural.fileids()\n",
    "    results = []\n",
    "    \n",
    "   # if Lower:\n",
    "       # SsQry = [word.lower() for word in SsQry]  # Lowercase SsQry\n",
    "\n",
    "    for fid in FIDs:\n",
    "        speech_text = set(nltk.corpus.inaugural.words(fid))\n",
    "        \n",
    "        \n",
    "        #if Lower:\n",
    "            #speech_text = [word.lower() for word in speech_text]  # Lowercase speech_text\n",
    "        \n",
    "       # if Stop:\n",
    "            #speech_text = [word for word in speech_text if word not in Stop]  # Remove stopwords\n",
    "            #SsQry = [word for word in SsQry if word not in Stop]  # Remove stopwords from SsQry\n",
    "        \n",
    "       # if Punct:\n",
    "            #speech_text = [word for word in speech_text if word not in Punct]  # Remove punctuation\n",
    "            #SsQry = [word for word in SsQry if word not in Punct]  # Remove punctuation from SsQry\n",
    "        \n",
    "        jaccard_score = JS(speech_text, SsQry, Lower, Stop, Punct)\n",
    "        results.append((fid.replace('.txt', ''), jaccard_score))\n",
    "    \n",
    "    df = pd.DataFrame(results, columns=['fid', 'JS'])\n",
    "    df.set_index('fid', inplace=True)\n",
    "    df_sorted = df.sort_values(by='JS', ascending=False)\n",
    "\n",
    "    return df_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-Obama</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-Obama</th>\n",
       "      <td>0.207624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-Clinton</th>\n",
       "      <td>0.191667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865-Lincoln</th>\n",
       "      <td>0.092593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829-Jackson</th>\n",
       "      <td>0.082674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793-Washington</th>\n",
       "      <td>0.022700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       JS\n",
       "fid                      \n",
       "2009-Obama       1.000000\n",
       "2013-Obama       0.207624\n",
       "1997-Clinton     0.191667\n",
       "...                   ...\n",
       "1865-Lincoln     0.092593\n",
       "1829-Jackson     0.082674\n",
       "1793-Washington  0.022700\n",
       "\n",
       "[59 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JS_Speech()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(JS_Speech().iloc[2,0], 0.19166666666666668)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fae0d3c217093f9e69f0397b4c357a9c",
     "grade": true,
     "grade_id": "JS_Speech_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 10 tests in 4.769s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test00 (__main__.Test_JS_Speech) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test01 (__main__.Test_JS_Speech) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test02 (__main__.Test_JS_Speech) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test03 (__main__.Test_JS_Speech) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test04 (__main__.Test_JS_Speech) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test05 (__main__.Test_JS_Speech) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test06 (__main__.Test_JS_Speech) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test07 (__main__.Test_JS_Speech) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test08 (__main__.Test_JS_Speech) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test09 (__main__.Test_JS_Speech) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "ae = npt.assert_equal\n",
    "@run_unittest\n",
    "class Test_JS_Speech(unittest.TestCase): # class with methods to test functionality of the JS() function\n",
    "    def test00(self): ae(JS_Speech().shape, (59,1))\n",
    "    def test01(self): ae(list(JS_Speech().T.columns[:2]), ['2009-Obama', '2013-Obama'])\n",
    "    def test02(self): ae(JS_Speech().T['2009-Obama'].values, 1)\n",
    "    def test03(self): ae(JS_Speech().iloc[2,0], 0.19166666666666668)\n",
    "    def test04(self): ae(JS_Speech(Lower=False, Stop={}).iloc[2,0], 0.23795620437956205)\n",
    "    def test05(self): ae(JS_Speech(Lower=False, Punct={}).iloc[2,0], 0.2004698512137823)\n",
    "    def test06(self): ae(JS_Speech(Lower=False, Stop={}, Punct={}).iloc[2,0], 0.24147933284989123)\n",
    "    def test07(self): ae(JS_Speech(Lower=True, Stop={}).iloc[2,0], 0.23637759017651575)\n",
    "    def test08(self): ae(JS_Speech(Lower=True, Punct={}).iloc[2,0], 0.19602977667493796)\n",
    "    def test09(self): ae(JS_Speech(Lower=True, Stop={}, Punct={}).iloc[2,0], 0.24009146341463414)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31762f9fdc547121692c906083f0fa58",
     "grade": false,
     "grade_id": "cell-1e41a5655861434a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Task 3\n",
    "\n",
    "In this task, you are given a Hamming distance UDF `HD()` and a sequence generating UDF `GenSeq()` similar to those you saw in the video. You will apply these functions in Task 3 described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37c84ba36ce10586946f6e72cef6d21b",
     "grade": false,
     "grade_id": "cell-36c940a907db2cea",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Hamming distance UDF\n",
    "HD = lambda s1='ab', s2='ad': sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2)) if len(s1)==len(s2) else np.inf\n",
    "\n",
    "def GenSeq(nLen=10, seed=int(0), LsElements=list('ACGT')):\n",
    "    '''Generate a list of sampled nLen objects listed in LsElements'''\n",
    "    if isinstance(seed, int):        # only integers >=0 are used for seeding\n",
    "        np.random.seed(abs(seed))      # seed random number generator (RNG) if integer seed is provided\n",
    "    return ''.join(np.random.choice(LsElements, nLen, replace=True))\n",
    "\n",
    "GenDNA = lambda nLen=5, seed=0: GenSeq(nLen, seed, list('ACGT'))\n",
    "sQry = GenDNA(100, seed=0)\n",
    "sTgt = GenDNA(200, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "650ff082b0e234cdfc48a441bd9d5d46",
     "grade": false,
     "grade_id": "cell-7456a3322dff361b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Complete the UDF `Marker()`, which takes two same-length strings, query `sQry` and target `sTgt`, and computes returns a \"**marked**\" string `sTgt` where the characters matching to corresponding characters in `sQry` are replaced with `_`. This function is helpful to visually highlight unmatched elements in string `sTgt`. If string lengths differ, UDF returns `''`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27c14423629df34a3989d276d7968127",
     "grade": false,
     "grade_id": "Marker_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def Marker(sQry='ACGG', sTgt='ACGT'):\n",
    "    '''Compares two same-length strings and replaces a character in sTgt with '_' \n",
    "        if it matches the corresponding character in sQry. Else leaves the character as is.\n",
    "    Inputs:\n",
    "      sQry, sTgt: target and query strings\n",
    "    Returns: a string with length of sTgt with some characters replaced with '_'.\n",
    "      If sTgt and sQry have different lengths, UDF returns '' (empty string)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return sOut  # return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Marker(sQry='ACGG', sTgt='ACGT'):\n",
    "    if len(sQry) != len(sTgt):\n",
    "        return ''\n",
    "    \n",
    "    sOut = ''.join(['_' if ch == sQry[i] else ch for i, ch in enumerate(sTgt)])\n",
    "    return sOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'___T'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Marker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "864846cbc34c382fc06c107809d4d2c7",
     "grade": true,
     "grade_id": "Marker_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 6 tests in 0.001s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test00 (__main__.Test_Marker) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test01 (__main__.Test_Marker) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test02 (__main__.Test_Marker) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test03 (__main__.Test_Marker) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test04 (__main__.Test_Marker) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test05 (__main__.Test_Marker) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "ae = npt.assert_equal\n",
    "@run_unittest\n",
    "class Test_Marker(unittest.TestCase): # class with methods to test functionality of the JS() function\n",
    "    def test00(self): ae(Marker(sQry='ACGG', sTgt='ACGT'), '___T')\n",
    "    def test01(self): ae(Marker(sQry='ACGT', sTgt='ACGT'), '____')\n",
    "    def test02(self): ae(Marker(sQry='ACGGA', sTgt='ACGT'), '')\n",
    "    def test03(self): ae(Marker(sQry='AGT', sTgt='ACGT'), '')\n",
    "    def test04(self): ae(Marker(GenDNA(20, seed=0), GenDNA(20, seed=0)), '____________________')\n",
    "    def test05(self): ae(Marker(GenDNA(20, seed=0), GenDNA(20, seed=1)), 'C_A__C_CTAAC__C_GC_A')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b4130beffb265c2e16a7f1b6a8cc992",
     "grade": false,
     "grade_id": "cell-f139500fc8791c79",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Task 4\n",
    "\n",
    "Now, apply `HD()` and `Marker()` to create UDF `RankHD()`, which takes two strings: query `sQry` and target `sTgt`, of varying lengths. Then, for each substring in `sTgt` with length equal to `len(sQry)`, compute the Hamming distance and the corresponding marked string. This can be done by looping over all substrings in `sTgt`, if such substrings exist. Package the results into two dataframe columns, as described in the function docstring below. \n",
    "\n",
    "This function can be used to find the closest match to `sQry` inside the `sTgt` and is often used by professionals to rank viral samples by their similarity or distance to the bank of known viral strings (which can have varying length samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f05f8fd3f88d59beda05b17df5e26d7",
     "grade": false,
     "grade_id": "RankHD_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def RankHD(sQry='ACGT', sTgt='ACTGTCT'):\n",
    "    '''This UDF uses HD() to rank each substring in sTgt \n",
    "        of length N=len(sQry) with its HD to sQry. \n",
    "        To do that you can iterate over each N-length substrings (i.e. Samples).\n",
    "        For each Sample compute HD(sQry, Sample) and Marker(sQry, Sample).\n",
    "        \n",
    "        #############  Marker(Sample, sQry)\n",
    "        For example, sTgt='ACTGTCT' has 4 Samples of length 4: ACTG,CTGT,TGTC,GTCT\n",
    "        and Marker('ACGT', 'ACTG') returns '__GT'\n",
    "    Inputs:\n",
    "      sQry, sTgt: strings of not necessarily same length.\n",
    "    Returns:\n",
    "      dataframe of shape M x 2 with columns 'HD' (rank or Hamming distance)\n",
    "      and 'Sample', which contains \n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return df # return dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Marker(sQry='ACGG', sTgt='ACGT'):\n",
    "    if len(sQry) != len(sTgt):\n",
    "        return ''\n",
    "    \n",
    "    sOut = ''.join(['_' if ch == sQry[i] else ch for i, ch in enumerate(sTgt)])\n",
    "    return sOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'___T'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Marker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def RankHD(sQry='ACGT', sTgt='ACTGTCT'):\n",
    "    #if len(sQry) > len(sTgt):\n",
    "       # return pd.DataFrame(columns=['HD', 'Sample'])\n",
    "    \n",
    "    N = len(sQry)  # length of query\n",
    "    results = []\n",
    "\n",
    "    for i in range(N):\n",
    "        sample = sTgt[i:i + N]  # Extract substring of length N\n",
    "        hd = HD(sample, sQry)  # Calculate Hamming distance\n",
    "        marker = Marker(sample, sQry, ) \n",
    "        #print(sQry, sample, marker, hd)\n",
    "        \n",
    "        # Append the result (Hamming distance and marked sample)\n",
    "        results.append((hd, marker))\n",
    "    \n",
    "    df = pd.DataFrame(results, columns=['HD', 'Sample'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HD</th>\n",
       "      <th>Sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>__GT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>AC__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>ACGT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ACG_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HD Sample\n",
       "0   2   __GT\n",
       "1   2   AC__\n",
       "2   4   ACGT\n",
       "3   3   ACG_"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RankHD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ACG_', 'ATCATTTTCT')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(RankHD(sQry, sTgt).iloc[-1,1][:10], 'ATCATTTTCT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aea49e98f272fb0398ac1b4af36681e6",
     "grade": true,
     "grade_id": "RankHD_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 9 tests in 0.015s\n",
      "\n",
      "\u001b[1m\u001b[31mFAILED (failures=7)\u001b[0m\n",
      "test00 (__main__.Test_RankHD) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test01 (__main__.Test_RankHD) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test02 (__main__.Test_RankHD) ... \u001b[1m\u001b[31mFAIL\u001b[0m\n",
      "test03 (__main__.Test_RankHD) ... \u001b[1m\u001b[31mFAIL\u001b[0m\n",
      "test04 (__main__.Test_RankHD) ... \u001b[1m\u001b[31mFAIL\u001b[0m\n",
      "test05 (__main__.Test_RankHD) ... \u001b[1m\u001b[31mFAIL\u001b[0m\n",
      "test06 (__main__.Test_RankHD) ... \u001b[1m\u001b[31mFAIL\u001b[0m\n",
      "test07 (__main__.Test_RankHD) ... \u001b[1m\u001b[31mFAIL\u001b[0m\n",
      "test08 (__main__.Test_RankHD) ... \u001b[1m\u001b[31mFAIL\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "\u001b[1m\u001b[31mFAIL: test02 (__main__.Test_RankHD)\u001b[0m\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-75-3b60f044a55f>\", line 7, in test02\n",
      "    def test02(self): ae(RankHD().iloc[-1,:].values.tolist(), [4, 'ACGT'])\n",
      "  File \"/home/codio/.local/lib/python3.6/site-packages/numpy/testing/_private/utils.py\", line 337, in assert_equal\n",
      "    assert_equal(actual[k], desired[k], 'item=%r\\n%s' % (k, err_msg), verbose)\n",
      "  File \"/home/codio/.local/lib/python3.6/site-packages/numpy/testing/_private/utils.py\", line 421, in assert_equal\n",
      "    raise AssertionError(msg)\n",
      "AssertionError: \n",
      "Items are not equal:\n",
      "item=0\n",
      "\n",
      " ACTUAL: 3\n",
      " DESIRED: 4\n",
      "\n",
      "======================================================================\n",
      "\u001b[1m\u001b[31mFAIL: test03 (__main__.Test_RankHD)\u001b[0m\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-75-3b60f044a55f>\", line 8, in test03\n",
      "    def test03(self): ae(RankHD(sQry='ACTGTCT', sTgt='ACGT').shape, (0, 2))\n",
      "  File \"/home/codio/.local/lib/python3.6/site-packages/numpy/testing/_private/utils.py\", line 337, in assert_equal\n",
      "    assert_equal(actual[k], desired[k], 'item=%r\\n%s' % (k, err_msg), verbose)\n",
      "  File \"/home/codio/.local/lib/python3.6/site-packages/numpy/testing/_private/utils.py\", line 421, in assert_equal\n",
      "    raise AssertionError(msg)\n",
      "AssertionError: \n",
      "Items are not equal:\n",
      "item=0\n",
      "\n",
      " ACTUAL: 7\n",
      " DESIRED: 0\n",
      "\n",
      "======================================================================\n",
      "\u001b[1m\u001b[31mFAIL: test04 (__main__.Test_RankHD)\u001b[0m\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-75-3b60f044a55f>\", line 9, in test04\n",
      "    def test04(self): ae(RankHD(sQry, sTgt).shape, (101, 2))\n",
      "  File \"/home/codio/.local/lib/python3.6/site-packages/numpy/testing/_private/utils.py\", line 337, in assert_equal\n",
      "    assert_equal(actual[k], desired[k], 'item=%r\\n%s' % (k, err_msg), verbose)\n",
      "  File \"/home/codio/.local/lib/python3.6/site-packages/numpy/testing/_private/utils.py\", line 421, in assert_equal\n",
      "    raise AssertionError(msg)\n",
      "AssertionError: \n",
      "Items are not equal:\n",
      "item=0\n",
      "\n",
      " ACTUAL: 4\n",
      " DESIRED: 101\n",
      "\n",
      "======================================================================\n",
      "\u001b[1m\u001b[31mFAIL: test05 (__main__.Test_RankHD)\u001b[0m\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-75-3b60f044a55f>\", line 10, in test05\n",
      "    def test05(self): ae(RankHD(sQry, sTgt).iloc[0,0], 60)\n",
      "  File \"/home/codio/.local/lib/python3.6/site-packages/numpy/testing/_private/utils.py\", line 421, in assert_equal\n",
      "    raise AssertionError(msg)\n",
      "AssertionError: \n",
      "Items are not equal:\n",
      " ACTUAL: 2\n",
      " DESIRED: 60\n",
      "\n",
      "======================================================================\n",
      "\u001b[1m\u001b[31mFAIL: test06 (__main__.Test_RankHD)\u001b[0m\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-75-3b60f044a55f>\", line 11, in test06\n",
      "    def test06(self): ae(RankHD(sQry, sTgt).iloc[-1,0], 88)\n",
      "  File \"/home/codio/.local/lib/python3.6/site-packages/numpy/testing/_private/utils.py\", line 421, in assert_equal\n",
      "    raise AssertionError(msg)\n",
      "AssertionError: \n",
      "Items are not equal:\n",
      " ACTUAL: 3\n",
      " DESIRED: 88\n",
      "\n",
      "======================================================================\n",
      "\u001b[1m\u001b[31mFAIL: test07 (__main__.Test_RankHD)\u001b[0m\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-75-3b60f044a55f>\", line 12, in test07\n",
      "    def test07(self): ae(RankHD(sQry, sTgt).iloc[0,1][:10], 'A_CATT__CT')\n",
      "  File \"/home/codio/.local/lib/python3.6/site-packages/numpy/testing/_private/utils.py\", line 421, in assert_equal\n",
      "    raise AssertionError(msg)\n",
      "AssertionError: \n",
      "Items are not equal:\n",
      " ACTUAL: '__GT'\n",
      " DESIRED: 'A_CATT__CT'\n",
      "\n",
      "======================================================================\n",
      "\u001b[1m\u001b[31mFAIL: test08 (__main__.Test_RankHD)\u001b[0m\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-75-3b60f044a55f>\", line 13, in test08\n",
      "    def test08(self): ae(RankHD(sQry, sTgt).iloc[-1,1][:10], 'ATCATTTTCT')\n",
      "  File \"/home/codio/.local/lib/python3.6/site-packages/numpy/testing/_private/utils.py\", line 421, in assert_equal\n",
      "    raise AssertionError(msg)\n",
      "AssertionError: \n",
      "Items are not equal:\n",
      " ACTUAL: 'ACG_'\n",
      " DESIRED: 'ATCATTTTCT'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "ae = npt.assert_equal\n",
    "@run_unittest\n",
    "class Test_RankHD(unittest.TestCase): # class with methods to test functionality of the JS() function\n",
    "    def test00(self): ae(RankHD().shape, (4, 2))\n",
    "    def test01(self): ae(RankHD().iloc[0,:].values.tolist(), [2, '__GT'])\n",
    "    def test02(self): ae(RankHD().iloc[-1,:].values.tolist(), [4, 'ACGT'])\n",
    "    def test03(self): ae(RankHD(sQry='ACTGTCT', sTgt='ACGT').shape, (0, 2))\n",
    "    def test04(self): ae(RankHD(sQry, sTgt).shape, (101, 2))\n",
    "    def test05(self): ae(RankHD(sQry, sTgt).iloc[0,0], 60)\n",
    "    def test06(self): ae(RankHD(sQry, sTgt).iloc[-1,0], 88)\n",
    "    def test07(self): ae(RankHD(sQry, sTgt).iloc[0,1][:10], 'A_CATT__CT')\n",
    "    def test08(self): ae(RankHD(sQry, sTgt).iloc[-1,1][:10], 'ATCATTTTCT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
