{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0534c03a367c968c9b736653068fe04",
     "grade": false,
     "grade_id": "cell-1047a4d68a7ced59",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# **Part Two of the Course Project**\n",
    "<span style=\"color:black\">In this project, you will identify common topics among 59 presidential inaugural speeches using Latent Dirichlet Allocation (LDA) built on a term frequency-inverse document frequency (TF-IDF) document term matrix (DTM).\n",
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31bd4a91e75fa9d1545fb834069e181c",
     "grade": false,
     "grade_id": "cell-38585629de5c87af",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# **Setup**\n",
    "Reset the Python environment to clear it of any previously loaded variables, functions, or libraries. Then, load the relevant libraries and speech document file IDs, containing an inauguration year and the president's last name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb65fc0aa3b9b1a5f71641b81bad9b72",
     "grade": false,
     "grade_id": "cell-1d50f23e1b2d5271",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from IPython.core.interactiveshell import InteractiveShell as IS\n",
    "IS.ast_node_interactivity = \"all\"    # allows multiple outputs from a cell\n",
    "import numpy as np, pandas as pd, nltk, plotly.express as px, seaborn as sns, matplotlib.pyplot as plt, scipy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from numpy.testing import assert_equal as eq, assert_almost_equal as aeq\n",
    "import unittest\n",
    "from colorunittest import run_unittest\n",
    "\n",
    "_ = nltk.download(['inaugural'], quiet=True)\n",
    "FIDs = nltk.corpus.inaugural.fileids()[:59]  # load file IDs (incl. 2021-Biden). This list grows over years\n",
    "print(FIDs[-5:])   # a few most recent presidential speech file names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f911b794f5e1817fcdf6fca85fc13fb",
     "grade": false,
     "grade_id": "cell-29b09895801b94dc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Next, load all speeches into `LsDocs` as a list of string documents. Below you will find a few recent speech excerpts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c60397d69add339ae614a2a48f47e348",
     "grade": false,
     "grade_id": "cell-5ccb02095526530c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "LsDocs = [nltk.corpus.inaugural.raw(fid) for fid in FIDs]\n",
    "[s[:100]+'...' for s in LsDocs[-5:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Building Topics\n",
    " \n",
    "Next, you will complete the `GetDTM()` function to compute the TF-IDF DTM. Use the following default parameters for the [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) object:\n",
    " \n",
    "<ol>\n",
    "<span style=\"color:black\"><li>\n",
    "Specify `ngram_range=(1,3)` to pick 1-gram, 2-gram, and 3-gram word sequences. The inaugural speeches are likely to contain these meaningful combinations. E.g., <code>'Vice President Cheney'</code>, <code>'Vice President'</code>, <code>'Mr. Chief Justice'</code>, <code>'Chief Justice'</code>, <code>'Thank you'</code>, <code>'United States'</code>, <code>'United States America'</code>, and so on. Shorter and longer grams can also be worth exploring, but longer grams will exponentially increase the count of considered keywords (or collocations) for vocabulary. \n",
    "    </li>\n",
    "<span style=\"color:black\"><li>\n",
    "Specify <code>stopwords='english'</code> to remove English stopwords. </li>\n",
    "<span style=\"color:black\"><li>\n",
    "Specify <code>min_df=0.01</code> to drop the least frequent words and collocations. </li>\n",
    "<span style=\"color:black\"><li>\n",
    "Specify <code>max_df=0.99</code> to drop any words and collocations that appear across all documents; hence, are also considered to be stop words). </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c31cdfe7291d5d1c949aad1c93948523",
     "grade": false,
     "grade_id": "GetDTM_ans",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def GetDTM(LsDocs=[''], ngram_range=(1,3), stop_words='english', min_df=0.01, max_df=0.99) -> scipy.sparse.csr_matrix:\n",
    "    '''Builds TF-IDF document term matrix (DTM) for LsDocs list of string documents.\n",
    "    It builds TfidfVectorizer() object with the input parameters and calls fit_transform() method on LsDocs.\n",
    "    Inputs:\n",
    "        LsDocs: list of string documents used to build vocabulary and resulting DTM\n",
    "        See TfidfVectorizer documentation for other parameters.            \n",
    "    Returns:\n",
    "        smDTM: TF-IDF document term matrix in sparse matrix format with n document rows and m word columns\n",
    "        tv: instantiated TfidfVectorizer object (which contains .vocabulary property)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return smDTM, tv\n",
    "\n",
    "smDTM, tv = GetDTM(LsDocs)  # build a TF-IDF DTM in sparse matrix format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tests will verify the correctness of your function implementation and help you troubleshoot your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db27158cf47c10fc4a5cf54885a80e9a",
     "grade": false,
     "grade_id": "cell-ac382ccad890ef4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@run_unittest\n",
    "class test_GetDTM(unittest.TestCase):\n",
    "    def test_00(self): eq(type(smDTM), scipy.sparse.csr.csr_matrix)\n",
    "    def test_01(self): eq(smDTM.shape, (59, 119141))  # verify shape of sparse matrix. It should have 59 rows (one for each speech document)\n",
    "    def test_02(self): eq(smDTM.nnz, 152443)          # verify count of non zero values\n",
    "    def test_03(self): aeq(smDTM.min(), 0)            # verify min value in DTM\n",
    "    def test_04(self): aeq(smDTM.max(), 0.17925725764535508)  # verify max value in DTM\n",
    "    def test_05(self): aeq(smDTM.sum(), 2769.289958141047)    # verify sum of all values\n",
    "    def test_06(self): eq(type(tv.vocabulary_), dict)         # verify dictionary type of vocabulary\n",
    "    def test_07(self): eq(len(tv.vocabulary_), 119141)        # verify total count of words in a dictionary\n",
    "    def test_08(self): eq(np.mean(list(tv.vocabulary_.values())), 59570.0)  # verify average count per word in a dictionary\n",
    "    def test_09(self): eq(sorted(tv.vocabulary_.keys())[-3:], ['zone', 'zone extending', 'zone extending degrees']) # a few words in a dictionary      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "787672b9309238eed1f0020fb1c15aad",
     "grade": true,
     "grade_id": "GetDTM_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST & AUTOGRADE CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aefaf3b9b2608fb41ffd68b00daa6b76",
     "grade": false,
     "grade_id": "cell-fc5060ac6a8188d7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<span style=\"color:black\">Next, complete the `GetLDA()` function, which consumes the sparse `smDTM` matrix, uses the [`LatentDirichletAllocation`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) object, and returns a dense LDA matrix wrapped as a Pandas DataFrame. Use the default parameters specified in the interface of `GetLDA()` when instantiating the LDA object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e878c99328dc22906dc01bbb2db6a760",
     "grade": false,
     "grade_id": "GetLDA_ans",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def GetLDA(smDTM, nTopics=10, random_state=0) -> pd.DataFrame:\n",
    "    '''Builds a dataframe of weights for documents (rows) and topics (columns).\n",
    "    First create LatentDirichletAllocation() object with the suitable given input parameters. \n",
    "    Then call fit_transform() method on the DTM you built earlier. \n",
    "    Wrap result as a dataframe before returning.\n",
    "    Inputs:\n",
    "        smDTM: TF-IDF DTM in sparse matrix format\n",
    "        nTopics: number of topic components requred by LatentDirichletAllocation\n",
    "        random_state: random number generator's seed to ensure reproducibility\n",
    "    Return:\n",
    "        LDA matrix wrapped as a dataframe.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return lda, dfLDA\n",
    "\n",
    "lda, dfLDA = GetLDA(smDTM)  # build LDA matrix wrapped as Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7181b94b86681160cdd09a3752d57246",
     "grade": false,
     "grade_id": "cell-18db5f77e0e701e4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The following cell verifies your implementation by checking key elements and statistics of the resulting `dfLDA` dataframe. You can also study these tests to troubleshoot any failing tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f10134d38ea443a61a9752f3c710efae",
     "grade": false,
     "grade_id": "cell-713fe4a15afcdd46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@run_unittest\n",
    "class test_GetLDA(unittest.TestCase):\n",
    "    def test_00(self): eq(type(dfLDA), pd.DataFrame)                  # verify dataframe structure\n",
    "    def test_01(self): eq(dfLDA.shape, (59, 10))                      # verify shape of the object\n",
    "    def test_02(self): aeq(dfLDA.min().min(), 0.0012051900213249666)  # verify smallest value in dataframe\n",
    "    def test_03(self): aeq(dfLDA.max().max(), 0.9867392361247341)     # verify largest value in dataframe\n",
    "    def test_04(self): aeq(dfLDA.sum().sum(), 59.0)                   # sum of all weights  should equal # of documents\n",
    "    def test_05(self): aeq(dfLDA.iloc[0,:3].values, [0.00248298, 0.00248299, 0.00248301]) # first 3 values in the top row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3ab7de9e1b8862bd89a4e947096cb11",
     "grade": true,
     "grade_id": "GetLDA_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST & AUTOGRADE CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5454f7bfa88e377d6635b20049fd613b",
     "grade": false,
     "grade_id": "cell-6bfb9f444c47d8b7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Visualizing Topics\n",
    "\n",
    "<span style=\"color:black\">The next few cells use your LDA dataframe to build informative visualizations. First, you will need to show topic weights for each presidential speech in a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b1fc66a87862630b343fb6c9e675cac",
     "grade": false,
     "grade_id": "cell-096d2f4a7298ff2c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "dfLDA['id'] = [s.replace('.txt','') for s in FIDs]\n",
    "dfLDA['Post'] = [f'[{i}], ' + s[:70] + '...' for i, s in enumerate(LsDocs)]\n",
    "cm = sns.light_palette(\"brown\", as_cmap=True)\n",
    "dfLDA.sort_index(ascending=False).style.background_gradient(cmap=cm, axis=1).set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb122310182df9ae1facdc86089f1fdf",
     "grade": false,
     "grade_id": "cell-d7eccb6f41520e25",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The bar plot below illustrates that topic 4 is the most common overall as it is contained within 16 speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ec2a6a95ec0952d23a30db6f221e430",
     "grade": false,
     "grade_id": "cell-9e3a858aa766e9fe",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "dfLDA.select_dtypes(include=['float64']).T.idxmax().value_counts().plot.barh(grid=True, title='Counts of documents in largest topics');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2ab6b908eab4b67596ba1ca08004f47",
     "grade": false,
     "grade_id": "cell-717226801fc3cf36",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "You will now plot the different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "351d9448b3958c75147f67b3434734e4",
     "grade": false,
     "grade_id": "cell-6c79c915f1541249",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def PlotTopics(model, LsVocab, nTopWords=20, nCols=5):\n",
    "    import math\n",
    "    nRows = math.ceil(len(model.components_) / nCols)         # round up\n",
    "    fig, axes = plt.subplots(nrows=nRows, ncols=nCols, figsize=(25, nTopWords*nRows/4), sharex=True)  # create a row of plot panels\n",
    "    axes = axes.flatten()                                     # change panel indexing from 2D to 1D\n",
    "    for nTpcIx, vTpc in enumerate(model.components_):         # topic index and word weights array\n",
    "        aTopWordsIx = vTpc.argsort()[:-nTopWords - 1:-1]      # find indices of important words in a topic\n",
    "        LsTopWords = [LsVocab[i] for i in aTopWordsIx]        # find top words in a topic\n",
    "        axes[nTpcIx].barh(LsTopWords, vTpc[aTopWordsIx])      # plot horizontal bars\n",
    "        axes[nTpcIx].set_title(f'Topic {nTpcIx}')             # set title of each panel\n",
    "        axes[nTpcIx].invert_yaxis()                           # flip y axis\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "PlotTopics(lda, tv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of top words and keywords among topics suggests that topic 4 relates to praises of the nation and its citizens, while not touching on the topics of world and war. The topic 5 focuses on a bright future and opportunities.\n",
    "\n",
    "The perplexity and log-likelihood metrics are typically used to automatically tune hyper parameters, such as the number of topics, preprocessing, etc. You could also accomplish this by choosing the parameters that increase log-likelihood, making it less negative, and lower perplexity. In fact, try different hyperparameters to see if you can improve these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Log Likelihood:\\t{lda.score(smDTM):.1f}')\n",
    "print(f'Perplexity:\\t{lda.perplexity(smDTM):.1f}')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
