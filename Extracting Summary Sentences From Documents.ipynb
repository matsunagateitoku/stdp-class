{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f23a31e9965e5eb08ce3bd58e44ef9c",
     "grade": false,
     "grade_id": "cell-d38294fc45c04a89",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# **Part Three of the Course Project**\n",
    "In this project, you will build a graph of sentences from a U.S. President's inaugural speech and apply the [PageRank](https://networkx.org/documentation/networkx-1.7/reference/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html) algorithm to rank the sentences by their importance score, referred to as 'Rank' herein. You will compute correlation using the Gramian matrix built from TF-IDF document term matrix (DTM). The more a given sentance is \"correlated\" with other sentences, the greater its importance.\n",
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5edb47da6ab003f68d8740a0af1afc10",
     "grade": false,
     "grade_id": "cell-a1bf264bcba08db1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# **Setup**\n",
    "Reset the Python environment to clear it of any previously loaded variables, functions, or libraries. Then, in the following code cell, load the necessary packages and download the inaugural corpus of presidential speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6497fade5b5bb2049bf5affb7075c6bc",
     "grade": false,
     "grade_id": "package_loading",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from IPython.core.interactiveshell import InteractiveShell as IS\n",
    "IS.ast_node_interactivity = \"all\"    # allows multiple outputs from a cell\n",
    "import nltk, re, seaborn as sns, matplotlib.pyplot as plt, pandas as pd, numpy as np, networkx as nx, unittest\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from numpy.testing import assert_equal as eq, assert_almost_equal as aeq\n",
    "import unittest\n",
    "from colorunittest import run_unittest\n",
    "\n",
    "_ = nltk.download(['punkt', 'inaugural'], quiet=True)  # load punctuation set and Gutenberg corporus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "784250e76b8c8b71974ffa1b6022ada9",
     "grade": false,
     "grade_id": "cell-57489d7887beb80e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68488b7e3bfca0f3a7190fe1067e0d6e",
     "grade": false,
     "grade_id": "cell-c27880bcc5437b68",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Task 1. Retrieve a list of sentences for the file ID\n",
    "\n",
    "You will complete the `GetSpeech()` function, which retrieves a list of sentences for the given file ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "603d9dabf9003f3c5b86dd4483437fdd",
     "grade": false,
     "grade_id": "GetSpeech_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def GetSpeech(fid:'nltk file id'='2017-Trump.txt') -> [str]:\n",
    "    '''Takes fid for an inaugural speech and retrieves the speech as string using raw() method.\n",
    "        Use nltk's sent_tokenize() to parse the string into sentences. \n",
    "        Leave other arguments of NLTK functions with their default values.\n",
    "    Input: NLTK's file id for inaugural speeches\n",
    "    Returns: list of string sentences from the presidential speech\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return LsSents\n",
    "\n",
    "LsSents = GetSpeech()\n",
    "LsSents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e065ad9d0fed3ed29c9ae4f7a1ed2819",
     "grade": false,
     "grade_id": "cell-9bdb3374891b3fb0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The following cell validates the output returned by your function. Evaluate the failed tests and correct your function accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c57dc10a21082cc648336813f4da50eb",
     "grade": true,
     "grade_id": "GetSpeech_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST & AUTOGRADE CELL\n",
    "@run_unittest\n",
    "class test_GetSpeech(unittest.TestCase):\n",
    "    def test_00(self): eq(type(LsSents), list)          # verify type of LsSents\n",
    "    def test_01(self): eq(type(LsSents[0]), str)        # verify type of first element of LsSents\n",
    "    def test_02(self): eq(len(LsSents), 90)             # verify count of sentences\n",
    "    def test_03(self): eq(len(LsSents[0].split()), 20)  # verify count of words in the first sentence\n",
    "    def test_04(self): eq(LsSents[0][:21], 'Chief Justice Roberts')  # verify count of words in the first sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Build a TF-IDF DTM\n",
    "\n",
    "Complete the `GetDTM()` function, which takes a list of sentences and builds a TF-IDF DTM with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e84c10292ebda09cf398fdaf42f38fbe",
     "grade": false,
     "grade_id": "GetDTM_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def GetDTM(LsSents=['']) -> pd.DataFrame:\n",
    "    '''Takes a list of sentence strings and returns a TF-IDF document term matrix (DTM) wrapped in a dataframe.\n",
    "        \n",
    "        Complete the following tasks:\n",
    "        1. Use default parameters of TfidfVectorizer() to create an object. \n",
    "        2. Then fit and transform the list of strings.\n",
    "        3. Finally, convert sparse matrix output to a numpy array and create a dataframe from it, \n",
    "           formatting as follows:\n",
    "            * Set the row index to just the first 50 characters of each sentence.\n",
    "            * Set the column index to the extracted vocabulary using tv.get_feature_names().   \n",
    "            \n",
    "    See TfidfVectorizer documentation or previous exercises on how to extract the vocabulary.\n",
    "    \n",
    "    Input: list of sentence strings from a document (such as inaugural speech)\n",
    "    Returns: DTM dataframe with rows as sentences and columns as vocabulary words.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return dfDTM\n",
    "    \n",
    "dfDTM = GetDTM(LsSents)\n",
    "\n",
    "def PlotDTM(df):\n",
    "    plt.figure(figsize = (30,3))\n",
    "    sns.heatmap(df.iloc[:10, :50].round(1), annot=True, cbar=False, cmap='Reds');\n",
    "PlotDTM(dfDTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1520406e93c6b22bcc3e4698552c97c2",
     "grade": false,
     "grade_id": "cell-c2d96281ea9e508c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "This cell evaluates the output of your function. Carefully evaluate any failed tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b652413ea4cb1f537da860057abc0530",
     "grade": true,
     "grade_id": "GetDTM_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST & AUTOGRADE CELL\n",
    "@run_unittest\n",
    "class test_GetDTM(unittest.TestCase):\n",
    "    def test_00(self): eq(type(dfDTM), pd.core.frame.DataFrame)    # verify whether function returns a data frame\n",
    "    def test_01(self): eq(dfDTM.shape, (90,534))                   # verify DTM dimensions\n",
    "    def test_02(self): eq('chief' in dfDTM.columns, True)          # verify if vocabulary contains 'chief'\n",
    "    def test_03(self): eq('infrastructure' in dfDTM.columns, True) # verify if vocabulary contains 'infrastructure'\n",
    "    def test_04(self): eq('administration' in dfDTM.columns, True) # verify if vocabulary contains 'administration'\n",
    "    def test_05(self): eq(max(len(s) for s in dfDTM.index), 50)    # verify length of sentence chunks used as indices\n",
    "    def test_06(self): eq((dfDTM!=0).sum().sum(), 1223)            # verify the count of non-zero weights\n",
    "    def test_07(self): eq((dfDTM==0).mean().mean(), 0.9745526425301707) # verify sparsity, i.e. fraction of zeros\n",
    "    def test_08(self): eq(dfDTM.mean().mean(), 0.006177271961164105)  # verify the mean of DTM\n",
    "    def test_09(self): eq(dfDTM.sum()['chief'], 0.20275874730023774)  # verify sum of weights for 'chief'\n",
    "    def test_10(self): eq(dfDTM.sum()['great'], 1.4017900224974988)   # verify sum of weights for 'great'\n",
    "    def test_11(self): eq(dfDTM.sum(axis=1)[0], 3.3081547440454915)   # verify sum of weights for the first sentence  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Retrieve a Gramian matrix of correlations \n",
    "\n",
    "Complete the `GetGramian()` function, which takes a DTM data frame and returns a Gramian matrix of correlations between sentences, then displayed in rows in `dfDTM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82f5e5f5bb097f5992f3f958fa0e6ce8",
     "grade": false,
     "grade_id": "GetGramian_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def GetGramian(dfDTM) -> np.array:\n",
    "    '''Return a symmetric Gramian (or Gram) matrix of correlations between sentences (as DataFrame object).\n",
    "    Input: DTM dataframe computed in GetDTM()\n",
    "    Returns: square/symmetric matrix of sentence correlations, which can be computed as a Gram matrix\n",
    "        wrapped into a DataFrame object\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return dfSim  # a data frame of a correlation matrix of sentence vectors from TF-IDF DTM\n",
    "\n",
    "dfSim = GetGramian(dfDTM)\n",
    "\n",
    "def PlotSimMatrix(df:pd.DataFrame) -> None:\n",
    "    '''Function to plot a heatmap of a correlation submatrix passed as a dataframe'''\n",
    "    plt.figure(figsize=(25, 4))\n",
    "    sns.heatmap(pd.DataFrame(df).round(1), annot=True, cbar=False, cmap='Reds');\n",
    "\n",
    "PlotSimMatrix(dfSim.iloc[:10,:30])  # plot top left submatrix as a heatmap (in color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell validates the outputs of your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9a40ea71ef539403d6eb4aa7821d776",
     "grade": true,
     "grade_id": "GetGramian_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST & AUTOGRADE CELL\n",
    "@run_unittest\n",
    "class Test_GetGramian(unittest.TestCase):\n",
    "    def test_00(self): eq(type(dfSim), pd.core.frame.DataFrame) # mSim should be a numpy array\n",
    "    def test_01(self): eq(dfSim.shape, (90,90)) # similarity matrix has square dimensions (# sentences by # of sentences)\n",
    "    def test_02(self): eq((np.diag(dfSim).round(0).astype(int)==1).sum(), 90) # diagonal values should all be ones\n",
    "    def test_03(self): self.assertTrue((dfSim.T==dfSim).all().all()) # similarity matrix should be symmetric, i.e. equal to its own transpose\n",
    "    def test_04(self): aeq(dfSim.sum().sum(), 531.6852815487657) # verify sum of all correlations\n",
    "    def test_05(self): aeq(dfSim.max().max(), 1) # verify max of all correlations\n",
    "    def test_06(self): aeq(dfSim.min().min(), 0) # verify min of all correlations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Build a NetworkX graph object\n",
    "\n",
    "Complete the `GetGraph()` function which takes a matrix of similarities &mdash; Gramian matrix as a NumPy array &mdash; and builds a NetworkX graph object from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09ae9b8ebdf0ff33b39c7794c53e85c3",
     "grade": false,
     "grade_id": "GetGraph_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def GetGraph(mSim: np.array) -> nx.classes.graph.Graph:\n",
    "    '''Creates a NetworkX graph object from Gramian matrix\n",
    "    Input: symmetric and square numpy 2D array of similarities\n",
    "    Returns: NetworkX graph object named 'Sentence Similarities'\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return G  # return NetworkX graph object\n",
    "\n",
    "G = GetGraph(dfSim.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell validates the outputs of your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80bd9d10de241d254130ae54defc9350",
     "grade": true,
     "grade_id": "GetGraph_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST & AUTOGRADE CELL\n",
    "@run_unittest\n",
    "class Test_GetGraph(unittest.TestCase):\n",
    "    def test_00(self): eq(type(G), nx.classes.graph.Graph)  # G should be a NetworkX graph object\n",
    "    def test_01(self): eq(len(G.nodes()), 90) # G should have a node for each of the sentences\n",
    "    def test_02(self): eq(G.name, 'Sentence Similarities')\n",
    "    def test_03(self): eq(len(G.edges), 2847) # verify connections (edges) between sentence nodes\n",
    "    def test_04(self): eq(G.degree(0), 71)  # node 0 should have 71 neighbors\n",
    "    def test_05(self): eq(max(G.neighbors(0)), 88)  # largest neighbor of node 0 is 88 (penultimate sentence)\n",
    "    def test_06(self): aeq(sum(dict(G.degree()).values())/len(G.degree()), 63.266666666666666)  # verify average degree   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Score the importance of each node \n",
    "\n",
    "Complete the `RankSents()` function, which takes a graph object `G` and applies PageRank algorithm to score the importance of each node, i.e., sentence, of this graph. The sentences and their scores are returned as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1814de369bad4d595b4f65df6790acba",
     "grade": false,
     "grade_id": "RankSents_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def RankSents(G: nx.classes.graph.Graph, LsSents=['']) -> pd.DataFrame:\n",
    "    '''Applies pagerank() method to graph G, retrieves items() to build a dataframe \n",
    "        with columns 'Rank' containing page rank for each sentence and 'Sent' containing the original sentence\n",
    "    Inputs:\n",
    "        G: NetworkX graph with sentences as nodes and edges as non-zero correlations between sentences\n",
    "        LsSents: original list of string sentences\n",
    "    Returns: a dataframe with columns Rank and Sent, sorted in decreasing order by column Rank\n",
    "        Index is the original counter of sentences starting from 0\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return dfPgRnk  # return dataframe of ranked sentences ordered by decreasing 'Rank' column\n",
    "    \n",
    "dfPgRnk = RankSents(G, LsSents)\n",
    "\n",
    "cm = sns.light_palette(\"brown\", as_cmap=True)\n",
    "dfPgRnk.sort_values('Rank', ascending=False)[:10].style.background_gradient(cmap=cm).set_precision(3)   # show top ranked sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell validates the outputs of your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0728c3848f4ca58aa37fc9b047580199",
     "grade": true,
     "grade_id": "RankSents_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST & AUTOGRADE CELL\n",
    "@run_unittest\n",
    "class Test_RankSents(unittest.TestCase):\n",
    "    def test_00(self): eq(type(dfPgRnk), pd.core.frame.DataFrame)   # the object should be a dataframe\n",
    "    def test_01(self): eq(dfPgRnk.shape, (90,2))                    # dataframe should have this shape\n",
    "    def test_02(self): eq(set(dfPgRnk.columns), {'Rank', 'Sent'})   # dataframe should have 2 columns\n",
    "    def test_03(self): eq(sum(dfPgRnk.index), 4005)                 # check sum of all indices\n",
    "    def test_04(self): aeq(dfPgRnk.sum()[0], 1)                     # check sum of all ranks\n",
    "    def test_05(self): aeq(dfPgRnk.max()[0], 0.017240936458358298)  # check largest rank\n",
    "    def test_06(self): aeq(dfPgRnk.min()[0], 0.0055880321854426245) # check smallest rank\n",
    "    def test_07(self): aeq(dfPgRnk.Rank.diff().max(), 0.008796499737907758)  # check descending order of sentences by their rank\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
