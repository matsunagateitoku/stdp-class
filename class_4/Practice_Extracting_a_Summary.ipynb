{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup**\n",
    " \n",
    "Reset the Python environment to clear it of any previously loaded variables, functions, or libraries. Then, import the libraries needed to complete the code Professor Melnikov presented in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from IPython.core.interactiveshell import InteractiveShell as IS\n",
    "IS.ast_node_interactivity = \"all\"    # allows multiple outputs from a cell\n",
    "import nltk, re, pandas as pd, heapq\n",
    "from collections import Counter\n",
    "_ = nltk.download(['punkt', 'stopwords'], quiet=True)\n",
    "LsStopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">\n",
    "\n",
    "# **Review**\n",
    " \n",
    "<span style=\"color:black\">In this notebook, you will create an extractive summary from a Wikipedia page on the [Python programming language](https://en.wikipedia.org/wiki/Python_(programming_language)). This page is stored as text in the `WikiPythonLang.txt` file. \n",
    "    \n",
    "<div style=\"margin-top: 20px; margin-bottom: 20px;\">\n",
    "<details style=\"border: 2px solid #ddd; margin-bottom: -2px;\">\n",
    "    <summary style=\"padding: 12px 15px; cursor: pointer; background-color: #eee;\">\n",
    "        <div id=\"button\" style=\"padding: 0px;\">\n",
    "            <font color=#B31B1B>▶ </font> \n",
    "            <b> More About:</b> Processing Raw Wikipedia Content\n",
    "        </div>\n",
    "    </summary>\n",
    "    <div id=\"button_info\" style=\"padding:10px\"> Raw Wikipedia content wraps content headers in <code>=</code> signs. Thus, <code>== History ==</code> is a section header 2, <code>=== Indentation ===</code> is a section header 3 and so on. It is worthwhile to investigate the text document to notice these and other patterns. Perform minor preprocessing by replacing <code>==+</code> regex patterns with periods and ensuring a single period between sentences. By doing so, NLTK will tokenize headers in the document as short individual, perhaps incomplete, sentences.\n",
    "    </div>\n",
    "</details>\n",
    "</div>\n",
    " \n",
    "<span style=\"color:black\">The basic algorithm for writing an abstract is not very complex and is reasonably effective. The algorithm uses word frequencies to rank the importance of each sentence and retrieves the top $n$ sentences. The algorithm does not penalize long sentences, which are likely to contain more words and more of the most frequent words. Thus, it can be improved in a number of ways by various sentence rank normalizations or thresholds on length of sentences returned in the summary. You will explore this in the Practice section below.\n",
    "    \n",
    "<span style=\"color:black\">Begin by preprocessing and tokenizing the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sDoc = '\\n'.join(list(open('WikiPythonLang.txt', 'r')))\n",
    "sDoc = re.sub('( \\.|\\.)+','.', re.sub('(==|\\n)+', '. ', sDoc)) # treates section headers as sentences\n",
    "LsSent = nltk.sent_tokenize(sDoc)        # show top few raw sentences\n",
    "LsSent[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\">Next, remove all non-ascii letters, tokenize/count words, and drop stopwords. The remaining word frequencies are displayed as a transposed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sDocClean = re.sub('[\\W\\d]', ' ', sDoc.lower())   # basic cleaning: remove all but ascii letters\n",
    "WordFreq = Counter(nltk.word_tokenize(sDocClean)) # tokenize and count words\n",
    "_ = [WordFreq.pop(s, None) for s in LsStopwords]  # remove stop words\n",
    "pd.DataFrame(WordFreq.items(), columns=['Word','Freq']).sort_values('Freq', ascending=False).set_index('Word').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\">Now you will build a function which accepts a list of sentences, a vocabulary dictionary with frequencies as values, and the number of sentences to return as arguments. Every sentence is associated with the total frequency of all words that are found in the vocabulary. Sentences are stored as keys of `SentScores` with scores as total frequencies of words therein. [`heapq()`](https://docs.python.org/3/library/heapq.html) is a fast and convenient way to retrieve `TopN` highest-score sentences and is an alternative to the more computationally expensive [`sorted()`](https://docs.python.org/3/library/functions.html#sorted) function. The top sentences are returned as a joined text summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Summarize(Sents=[], WordFreq={}, TopN=3) -> []:\n",
    "    SentScores = {}   # storage for sentences and their scores\n",
    "    for sent in Sents:\n",
    "        for word in nltk.word_tokenize(sent.lower()):  # parse a sentence into lower case word tokens\n",
    "            if word in WordFreq.keys(): \n",
    "                SentScores[sent] = SentScores.get(sent, 0) +  WordFreq[word]  # add frequency of the word to host sentence\n",
    "    LsTopSents = heapq.nlargest(TopN, SentScores, key=SentScores.get)     # find TopN scored sentences\n",
    "    return ' '.join(LsTopSents)\n",
    "\n",
    "print(Summarize(LsSent, WordFreq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">\n",
    "\n",
    "# **Optional Practice**\n",
    "\n",
    "Now, equipped with these concepts and tools, you will tackle a few related tasks.\n",
    "\n",
    "As you work through these tasks, check your answers by running your code in the *#check solution here* cell, to see if you’ve gotten the correct result. If you get stuck on a task, click the See **solution** drop-down to view the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Modify `Summarize()` to ignore any sentences longer than `MaxSentLen` parameter with default value of 20 words.\n",
    "\n",
    "<b>Hint:</b> A single line of code should suffice. It should tokenize a sentence and check whether the number of word tokens is smaller than the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=#606366>\n",
    "    <details><summary><font color=#B31B1B>▶ </font>See <b>solution</b>.</summary>\n",
    "<pre>\n",
    "def Summarize(Sents=[], WordFreq={}, TopN=3, MaxSentLen=20) -> []:\n",
    "    SentScores = {}   # storage for sentences and their scores\n",
    "    for sent in Sents:\n",
    "        for word in nltk.word_tokenize(sent.lower()):  # parse a sentence into lower case word tokens\n",
    "            if word in WordFreq.keys():\n",
    "                if len(sent.split(' ')) < MaxSentLen:  ##### ADDED CODE \n",
    "                    SentScores[sent] = SentScores.get(sent, 0) +  WordFreq[word]  # add frequency of the word to host sentence\n",
    "    LsTopSents = heapq.nlargest(TopN, SentScores, key=SentScores.get)     # find TopN scored sentences\n",
    "    return ' '.join(LsTopSents)\n",
    "\n",
    "print(Summarize(LsSent, WordFreq))\n",
    "</pre>\n",
    "</details> \n",
    "</font>\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
