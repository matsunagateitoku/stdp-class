{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup**\n",
    " \n",
    "Reset the Python environment to clear it of any previously loaded variables, functions, or libraries. Then, import the libraries needed to complete the code Professor Melnikov presented in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from IPython.core.interactiveshell import InteractiveShell as IS\n",
    "IS.ast_node_interactivity = \"all\"    # allows multiple outputs from a cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">\n",
    "\n",
    "# **Review**\n",
    "\n",
    "<span style=\"color:black\">In this notebook, you will rebuild the previous logistic regression model that uses the cosine similarity feature. The focus is on making and interpreting predictions from the model.\n",
    "\n",
    "<span style=\"color:black\">As before, you will load the appropriate packages and the word2vec model, `glove-wiki-gigaword-50.gz`. Note that you will use Word2Vec instead of FastText because the model loads 10x faster and is sufficient for this demostration. Define a cosine similarity function using cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, nltk, seaborn as sns, matplotlib.pyplot as plt\n",
    "import plotly.express as px, plotly.graph_objects as go\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.spatial.distance import cosine  # a cosine distance, not similarity\n",
    "CosSim = lambda x, y: 1 - cosine(x, y)     # convert distance to similarity\n",
    "\n",
    "# Dictionary-like object. key=word (string), value=trained embedding coefficients (array of numbers)\n",
    "%time wv = KeyedVectors.load_word2vec_format('glove-wiki-gigaword-50.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Balanced Classification Problem**\n",
    "\n",
    "Begin by loading the male and female names into lists, and drop names that are not found in word2vec. This downsamples the majority class, i.e. the class with a larger count of observations (or names). A procedure that results in equal count of observations from each class produces a **balanced classification problem**, where the observations from two classes are equally likely (without any information from the predictors). Thus, an accuracy that is higher than 50% indicates that the model performs better than random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = nltk.download(['names'], quiet=True)\n",
    "LsM = [name.strip().lower() for name in nltk.corpus.names.words('male.txt')]\n",
    "LsF = [name.strip().lower() for name in nltk.corpus.names.words('female.txt')]\n",
    "\n",
    "# Balance observations in two classes. So, a random draw has 50% chance of being from either class.\n",
    "np.random.seed(0)\n",
    "LsF = sorted(np.random.choice(LsF, size=len(LsM), replace=False))\n",
    "LsM2, LsF2 = [s for s in LsM if s in wv], [s for s in LsF if s in wv]\n",
    "LsM2 = np.random.choice(LsM2, size=min(len(LsM2), len(LsF2)), replace=False).tolist()\n",
    "LsF2 = np.random.choice(LsF2, size=min(len(LsM2), len(LsF2)), replace=False).tolist()\n",
    "print(f'{len(LsM2)} male names:  ', LsM2[:20])\n",
    "print(f'{len(LsF2)} female names:', LsF2[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functional Programming**\n",
    "\n",
    "Next, rewrite the `CosSim2Female()` function in a [functional](https://docs.python.org/3/howto/functional.html) style so that it returns a function. Doing this makes it easier to add features to the dataframe because you can use one functional for various query strings. Test the functional below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity to query function, i.e. it returns a function which can be further evaluated\n",
    "CS2Q = lambda sQuery='man': lambda sName: CosSim(wv[sName], wv[sQuery]) if sName in wv else 0 \n",
    "df = pd.DataFrame(LsF2 + LsM2, columns=['Name'])\n",
    "df['Y'] = [1] * len(LsF2) + [0] * len(LsM2)   # create numeric labels for names\n",
    "df['CS2F'] = df.Name.apply(CS2Q('feminine'))   # for each name compute cosine similarity to female query word\n",
    "df = df.sort_values('CS2F', ascending=False).set_index('Name')\n",
    "df.T.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the `'man'` query string gives a higher cosine similarity to the male name `'david'` than to `'kathy'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train Logistic Regression**\n",
    "\n",
    "Now you can split the dataset into train and validation samples, train the logistic regression model, and evaluate its validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX, vX, tY, vY = train_test_split(df.drop('Y', axis=1), df.Y, test_size=0.2, random_state=0)\n",
    "lr = LogisticRegression(random_state=0)   # create a model and (always) seed random number generator\n",
    "lr.fit(tX, tY)                            # fit a model to compute model parameters\n",
    "print(f'Accuracy = fraction of correct predictions: {lr.score(vX, vY):.3f}') # report out of sample accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package all results into a dataframe and add `isAccurate` to the rows where the prediction matches the observed class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfvXY = vX.copy()\n",
    "dfvXY['P[Y=0(male)|CS2F]'] = lr.predict_proba(vX)[:,0]\n",
    "dfvXY['P[Y=1(female)|CS2F]'] = lr.predict_proba(vX)[:,1]\n",
    "dfvXY['pY'] = pY = lr.predict(vX)\n",
    "dfvXY['vY'] = vY\n",
    "dfvXY['isAccurate'] = dfvXY.pY == dfvXY.vY\n",
    "dfvXY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Count Outputs**\n",
    "\n",
    "Using the `isAccurate` row, count the correctly and incorrectly classified outputs (i.e., correctly classified means that the predicted output matches the observed output, whether that value is 0 or 1; otherwise, it is misclassified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfvXY[['vY', 'pY']].value_counts().reset_index().rename(columns={0:'Counts'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two rows show correct classification counts, which add up to 517, and the bottom rows indicate the misclassified names, which sum up to 309. Overall, the accuracy is $517/(309+517)\\approx 0.626$ which is higher than the baseline accuracy of 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Confusion Matrix**\n",
    "\n",
    "Now you are ready to create a [confusion matrix](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix) from this dataframe. The confusion matrix places all correctly predicted counts on its diagonal and all incorrect predictions off its diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true=vY, y_pred=pY, labels=[0,1])\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without referring to the scikit-learn documentation, it is hard to tell whether the rows in this matrix correspond to observed classes and columns correspond to predicted classes. To improve the readability of the confusion matrix, label the dimensions and plot the matrix as an annotated heatmap using seaborn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "MsTxt = np.char.array([['TN','FP'], ['FN','TP']]) + '='\n",
    "MsTxt = MsTxt + cm.astype('str') + '; ' + (cm/cm.sum() * 100).round(1).astype('str') + '%'\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 2]\n",
    "ax = sns.heatmap(cm, annot=MsTxt, cbar=False, cmap='coolwarm', fmt='', annot_kws={\"fontsize\":20});\n",
    "ax.set_title('Confusion Matrix: counts and % of total count');\n",
    "ax.set(xlabel='Predicted labels', ylabel='True labels');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the matrix, you can see that there are 17.6% false positives, i.e., true negatives that the model reports as positives. Recall that you defined female gender as 1, which is now the \"positive\" class. The male gender label is 0 or negative class.\n",
    "\n",
    "scikit-learn has a function, [`plot_confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html), that does majority of this work in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "disp = plot_confusion_matrix(lr, vX, vY, display_labels=[0,1], cmap=plt.cm.Blues, normalize='all');\n",
    "disp.ax_.set_title('Confusion Matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">\n",
    "\n",
    "# **Optional Practice**\n",
    "Now, equipped with these concepts and tools, you will tackle a few related tasks.\n",
    "\n",
    "As you work through these tasks, check your answers by running your code in the *#check solution here* cell, to see if you’ve gotten the correct result. If you get stuck on a task, click the See **solution** drop-down to view the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    " \n",
    "Copy the `df` dataframe into `df1` and add a column `CS2lady`, which uses `CS2Q()` function to compute cosine similarity distances from the given name (in each row) to the target word `lady`.\n",
    "\n",
    "<b>Hint:</b> The code above should guide you through these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=#606366>\n",
    "    <details><summary><font color=#b31b1b>▶ </font>See <b>solution</b>.</summary>\n",
    "<pre>\n",
    "df1 = df.copy()\n",
    "df1['CS2lady'] = df.reset_index().Name.apply(CS2Q('lady')).values\n",
    "df1.T\n",
    "</pre>\n",
    "</details> \n",
    "</font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Split `df1` rows into train and test inputs and outputs and then train a logistic regression model with two input features, `CS2F` and `CS2lady`. Compute the out of sample (i.e. test) accuracy.\n",
    "\n",
    "<b>Hint:</b> Use the code above as your guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=#606366>\n",
    "    <details><summary><font color=#b31b1b>▶ </font>See <b>solution</b>.</summary>\n",
    "<pre>\n",
    "tX, vX, tY, vY = train_test_split(df1.drop('Y', axis=1), df1.Y, test_size=0.2, random_state=0)\n",
    "lr = LogisticRegression(random_state=0).fit(tX, tY)   # create a model; compute model parameters\n",
    "print(f'Accuracy = fraction of correct predictions: {lr.score(vX, vY):.3f}') # report out of sample accuracy\n",
    "</pre>\n",
    "</details> \n",
    "</font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    " \n",
    "Build a confusion matrix for the new model. Which true class shows the greatest improvement?\n",
    "\n",
    "<b>Hint:</b> Use the code above as your guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#606366>\n",
    "    <details><summary><font color=carnelian>▶ </font>See <b>solution</b>.</summary>\n",
    "<pre>\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "disp = plot_confusion_matrix(lr, vX, vY, display_labels=[0,1], cmap=plt.cm.Blues, normalize='all');\n",
    "disp.ax_.set_title('Confusion Matrix');</pre>\n",
    " \n",
    "The true female gender class (lower row of the matrix) was improved the most. Correct classification rose from 31% to 34%, while male gender class increased by only 1%.\n",
    "    </details> \n",
    "</font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "Try adding more cosine similarity features based on words that have strong gender bias, such as certain gender-associated professions, names, gender specific products and services. Add these one by one and keep only those features that improve your accuracy.\n",
    "\n",
    "This task is open-ended and does not have a single specific solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check solution here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
