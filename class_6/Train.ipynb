{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part Two of the Course Project**\n",
    "In this part of the course project, you will build and train a named entity recognition model, which recognizes movie related named entities. \n",
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup**\n",
    " \n",
    "First conditional random fields (CRF) and scikit-learn (SKL) libraries need to be aligned in compatible versioning. So, we lower SKL's version to 0.23.2 to avoid errors from SKL (until the CRF library is upgraded by their authors). Recall that CRF library allows non-numeric features in dictionary format and predicts the chunks and their labels simultaneously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U --force-reinstall scikit-learn==0.23.2 > Log   # SKL version compatible with CRF\n",
    "!pip freeze | grep learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the Python environment to clear it of any previously loaded variables, functions, or libraries. Then, import the libraries and corpora needed for this project. In this project you will use NLTK's parts of speech (POS) tagger, so `punk` and `averaged_perceptron_tagger` corpora are needed. The `warnings` library hides the `FutureWarnings` rising from the lower version of SKL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from IPython.core.interactiveshell import InteractiveShell as IS\n",
    "IS.ast_node_interactivity = 'all'\n",
    "import pandas as pd, numpy as np, nltk, unittest, numpy.testing as npt, sklearn_crfsuite as CRF, warnings, re\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn_crfsuite.metrics import flat_classification_report as rpt  # model's detailed metrics\n",
    "from colorunittest import run_unittest\n",
    "from collections import Counter\n",
    "eq, aeq = npt.assert_equal, npt.assert_almost_equal\n",
    "\n",
    "_ = nltk.download(['punkt', 'averaged_perceptron_tagger'], quiet=True) # silently load corpora from NLTK\n",
    "np.set_printoptions(linewidth=100, precision=4, edgeitems=20, suppress=True) \n",
    "pd.set_option('max_colwidth', 200, 'max_columns', 50, 'display.max_rows', 6)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # suppress FutureWarning warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two text files, *engtrain.bio.txt* and *engtest.bio.txt* (in the local drive of Jupyter Notebook (JN)) contain English train and test text queries in the movie domain. These [MIT Movie text files](https://groups.csail.mit.edu/sls/downloads/) are in BIO format, where each word is on its own line and sentences are separated by blank line. Each word is preceded by an IOB (inside-outside-beginning) style NE tags and a tab `\\t`, as shown in `sEW3` below. \n",
    " \n",
    "Your task will involve reading each BIO file into a list of lists of tuples in `(Word,NE_Tag)` format (example: list of `LTsWE3`-like elements) and then adding POS tags. The ready format will be a list of `LTsWPE3`-like elements shown below. Then a window of three words (and their attributes) will be featurized into a list of lists of dictionaries of features (key-value pairs), which will be fed to a CRF model for training. Both train and test file must be featured in the same way. Finally, featurized test sentences will be used to evaluate the quality of your model.\n",
    " \n",
    "Notice that SpaCy model cannot handle these NE tags out of the box. However, the CRF model allows you to train your own chunker/tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sEW3 = 'B-ACTOR\\tStallone\\nO\\tin\\nB-TITLE\\tRocky\\n\\n'; print(sEW3) # Bio format, string NE+Word, example with 3 tokens\n",
    "LTsWE3 = [('Stallone','B-ACTOR'), ('in','O'), ('Rocky','B-TITLE')]\n",
    "LTsWPE3 = [('Stallone','NN','B-ACTOR'), ('in','IN','O'), ('Rocky','NN','B-TITLE')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few lines from the training file that are printed without invisible symbols, `\\t` and `\\n`. The `strip()` removes any possible leading/trailing whitespace characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"engtrain.bio.txt\") as file: _ = ([print(next(file).strip()) for x in range(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few lines printed as a list of strings with the invisible characters. Each element is a line in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"engtrain.bio.txt\") as file: print([next(file) for x in range(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Load BIO File\n",
    " \n",
    "Complete `LoadBioFile()` function, which takes the name of the file in BIO (text) format and returns a list of lists of tuples. Each outer list's element is in the format of `LTsWE3` variable. Remember to strip each line of any leading/trailing whitespaces.\n",
    "\n",
    "For example, `BioFile2WE('engtrain.bio.txt')[0:2]` should return the following two tagged sentences: \n",
    "\n",
    "    [[('what', 'O'), ('movies', 'O'), ('star', 'O'), ('bruce', 'B-ACTOR'), ('willis', 'I-ACTOR')], [('show', 'O'), ('me', 'O'), ('films', 'O'), ('with', 'O'), ('drew', 'B-ACTOR'), ('barrymore', 'I-ACTOR'), ('from', 'O'), ('the', 'O'), ('1980s', 'B-YEAR')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ff3a017e13b2a37f3c5e1bac3ab3279",
     "grade": false,
     "grade_id": "BioFile2WE_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def BioFile2WE(FileName='') -> list(list((str, str,))):\n",
    "    '''Takes the name of BIO file, FileName, which is stored locally in the script's directory.\n",
    "    Returns: list of lists of tuples. Each tuple is in the format (word, NE_tag). \n",
    "    Note that BIO file has the reversed order on each line: NE_tag and word (separted by tab \\t)    '''\n",
    "    LLTsWE = [[('W','NE')]]   # desired output format to be build\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return LLTsWE\n",
    "\n",
    "tWE = BioFile2WE('engtrain.bio.txt')\n",
    "vWE = BioFile2WE('engtest.bio.txt')\n",
    "print('> ', tWE[0:2])\n",
    "print('> ', vWE[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c80c556bcd5c281b21b7dbab61eb593",
     "grade": true,
     "grade_id": "BioFile2WE_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "flat = lambda LLT: [T for LT in LLT for T in LT] # flatten container of containers, [[1,2],[3,4]] => [1,2,3,4]\n",
    "\n",
    "@run_unittest\n",
    "class Test_BioFile2WE(unittest.TestCase):\n",
    "    def test00(self): eq(type(tWE), list)  # check if train output is a list\n",
    "    def test01(self): eq(type(vWE), list)  # check if test output is a list\n",
    "    def test02(self): eq(all(type(e)==list for e in tWE), True) # tWE must contain lists only\n",
    "    def test03(self): eq(all(type(e)==list for e in vWE), True) # vWE must contain lists only\n",
    "    def test04(self): eq(all(type(t)==tuple for t in flat(tWE)), True) # inner list must contain only tuples\n",
    "    def test05(self): eq(all(type(t)==tuple for t in flat(vWE)), True) # inner list must contain only tuples\n",
    "    def test06(self): eq(all(type(s)==str for s in flat(flat(tWE))), True) # tuples must contain strings\n",
    "    def test07(self): eq(all(type(t)==str for t in flat(flat(vWE))), True) # tuples must contain strings\n",
    "    def test08(self): eq(tWE[0], [('what','O'),('movies','O'),('star','O'),('bruce','B-ACTOR'),('willis','I-ACTOR')])\n",
    "    def test09(self): eq(vWE[5], [('show','O'),('me','O'),('1980s','B-YEAR'),('action','B-GENRE'),('movies','O')])\n",
    "    def test10(self): eq(len(tWE), 9775) # number of training sentences\n",
    "    def test11(self): eq(len(vWE), 2443) # number of test sentences\n",
    "    def test12(self): eq(len(flat(tWE)), 99491) # number of training words\n",
    "    def test13(self): eq(len(flat(vWE)), 24686) # number of testing words\n",
    "    def test14(self): eq(Counter(e for w, e in flat(vWE) if e[0]=='B'), {'B-ACTOR': 812,'B-CHARACTER': 90,\n",
    "         'B-DIRECTOR': 456,'B-GENRE': 1117,'B-PLOT': 491,'B-RATING': 500,'B-RATINGS_AVERAGE': 451,'B-REVIEW': 56,\n",
    "         'B-SONG': 54,'B-TITLE': 562,'B-TRAILER': 30,'B-YEAR': 720})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project you will build a typical set of features to predict the following NE tags. Your goal might be to improve the model quality overall or to improve the quality of a specific NE tag. The performance of the model on some tag depends on the diversity of tagged words, number of examples, and the model's ability to fit and generalize to these examples. \n",
    " \n",
    "For example, one would expect genres to be from a small (say 30 to 40) set of words. You could create a feature containing the whole word, but this would explode the model in memory because it will memorize all words and will not generalize. Instead, it is more advantageous to find a small set of features (from the tagged and neighboring words) that keeps the model small, fast, and generalizable. \n",
    " \n",
    "Years and ratings are numbers, so some feature based on digit symbols might be helpful in predicting these two NE tags (this is just an intuition, i.e. a hypothesis yet to be tested, not a fact). \n",
    " \n",
    "The dataset has actor/director/character names in large numbers. While there are many examples, these names also vary widely and some name-recognizing features would be helpful. \n",
    " \n",
    "Plot, review and other textual phrases may be short sentences, with nouns, verbs, and adjectives, so POS tagging should help. This is what you need to build next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(Counter([e for sent in tWE for w,e in sent]), orient='index').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Add POS Tag\n",
    "\n",
    "Complete a UDF `InsertPOS()`, which takes a single sentence (any inner list from, say, `tWE`) and inserts a POS tag next to each word. It is then applied to each sentence to populate all POS tags, which are likely (not guaranteed) to be helpful in predicting NE tags.\n",
    "\n",
    "For example, the UDF converts the first train sentence from \n",
    "\n",
    "    [('what','O'), ('movies','O'), ('star','O'), ('bruce','B-ACTOR'), ('willis','I-ACTOR')]\n",
    "    \n",
    "to\n",
    "\n",
    "    [('what','WP','O'), ('movies','NNS','O'), ('star','VBP','O'), ('bruce','NN','B-ACTOR'), ('willis','NN','I-ACTOR')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4acb893f25d437ef062e94f6c41ea38a",
     "grade": false,
     "grade_id": "InsertPOS_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def InsertPOS(LTsWE=LTsWE3) -> list(list((str, str, str))):\n",
    "    '''Converts a sentence in WE format to the sentence in WPE format, \n",
    "        i.e. [(word, NE),...] -> [(word, POS, NE),...]\n",
    "    Use nltk.pos_tag() to generate POS tags for a sentence given as a list of tuples. \n",
    "    Then add these tags to their corresponding words, so as to convert (word, NE) into (word, POS, NE).\n",
    "    This UDF deals with one sentence only.\n",
    "    Hint: you can use for loops or several applications of zip(*...) function'''\n",
    "    LTsWPE = [('W','POS','NE')]   # desired output format to be build\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return LTsWPE\n",
    "\n",
    "%time tWPE = [InsertPOS(Sent) for Sent in tWE]\n",
    "%time vWPE = [InsertPOS(Sent) for Sent in vWE]\n",
    "tWPE[0]\n",
    "vWPE[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78451db62536e83fe7aa3bb09a1b5a65",
     "grade": true,
     "grade_id": "InsertPOS_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class Test_InsertPOS(unittest.TestCase):\n",
    "    def test00(self): eq(type(tWPE[0]), list)\n",
    "    def test01(self): eq(type(vWPE), list)\n",
    "    def test02(self): eq(all(type(e)==list for e in vWPE), True)            # vWPE must contain only lists\n",
    "    def test03(self): eq(all(type(t)==str for t in flat(tWPE[0])), True)    # tuples must contain strings only\n",
    "    def test04(self): eq(all(type(t)==tuple for t in flat(vWPE)), True)     # inner list must contain tuples only\n",
    "    def test05(self): eq(all(type(s)==str for s in flat(flat(tWPE))), True) # tuples must contain strings only\n",
    "    def test06(self): eq(all(type(t)==str for t in flat(flat(vWPE))), True) # tuples must contain strings only\n",
    "    def test07(self): eq(tWPE[0], [('what','WP','O'), ('movies','NNS','O'), ('star','VBP','O'), ('bruce','NN','B-ACTOR'), ('willis','NN','I-ACTOR')])\n",
    "    def test08(self): eq(vWPE[5], [('show','VB','O'), ('me','PRP','O'), ('1980s','CD','B-YEAR'), ('action','NN','B-GENRE'), ('movies','NNS','O')])\n",
    "    def test09(self): eq(len(tWPE), 9775) # number of training sentences\n",
    "    def test10(self): eq(len(vWPE), 2443) # number of test sentences\n",
    "    def test11(self): eq(len(flat(tWPE)), 99491) # number of training words\n",
    "    def test12(self): eq(len(flat(vWPE)), 24686) # number of testing words\n",
    "    def test13(self): eq(Counter(e for w, p, e in flat(vWPE) if e[0]=='B'), {'B-ACTOR': 812,'B-CHARACTER': 90,\n",
    "         'B-DIRECTOR': 456,'B-GENRE': 1117,'B-PLOT': 491,'B-RATING': 500,'B-RATINGS_AVERAGE': 451,'B-REVIEW': 56,\n",
    "         'B-SONG': 54,'B-TITLE': 562,'B-TRAILER': 30,'B-YEAR': 720})\n",
    "    def test14(self): eq(Counter(p for w, p, e in flat(vWPE) if p[0]=='V'), \n",
    "                 {'VB':865,'VBD':1193,'VBG':377,'VBN':1003,'VBP':452,'VBZ': 811})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "Complete `Case2Num()` to return a number associated with capitalization of UDF's argument `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w='123*@^$(*&)'\n",
    "w.lower()==w.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b291d0a811618e8474c6d8327735be6",
     "grade": false,
     "grade_id": "Case2Num_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def Case2Num(w='Word') -> int:\n",
    "    \"\"\" Ð¢akes a word (string) w and returns a number:\n",
    "    If w is not a string OR w in lower case is the same as w in upper case, return 3.\n",
    "        E.g. w='123*@^$(*&)'; w.lower()==w.upper() # returns TRUE. Casing makes no sense for numbers\n",
    "    0 if w contains low case letters. May help with plot descriptions.\n",
    "        E.g. you can lower-case w and compare it to the original w\n",
    "    1 if w contains capital case letters.\n",
    "        E.g. you can upper-case w and compare it to the original w\n",
    "    2 if w is a title case word. May help with proper names in English language, incl. week days, month names\n",
    "        E.g. you can title-case w and compare it to the original w\n",
    "    3 otherwise      \"\"\"\n",
    "    nFeatureValue = 0  # desired format of the output\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return nFeatureValue\n",
    "\n",
    "print(Case2Num('out0'), Case2Num('OUT1'), Case2Num('Out2'), Case2Num('oUt3')) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dc86892c650473b7e1604e04901bead",
     "grade": true,
     "grade_id": "GetCaseFeature_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class Test_Case2Num(unittest.TestCase):\n",
    "    def test00(self): eq(type(Case2Num('out0')), int)  \n",
    "    def test01(self): eq(Case2Num('out0'), 0)  \n",
    "    def test02(self): eq(Case2Num('OUT0'), 1)  \n",
    "    def test03(self): eq(Case2Num('Out2'), 2) \n",
    "    def test04(self): eq(Case2Num('oUt3'), 3) \n",
    "    def test05(self): eq(Case2Num(1), 3)      # return 3 for non strings \n",
    "    def test06(self): eq(Case2Num([]), 3)     # return 3 for non strings \n",
    "    def test07(self): eq(Case2Num(None), 3)   # return 3 for non strings \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "Complete `Featurize()`, which takes the word `w`, its POS tag, and its location `loc`. It then returns a dictionary of features with a feature name as a key and feature value as the value. Feature values need not be numeric for the CRF model.\n",
    "\n",
    "Example. The following four calls:\n",
    "\n",
    "    Featurize(w='7', POS='NNP', loc='')     # '7' is neither capital nor lower case. Key C is 3\n",
    "    Featurize(w='AOL', POS='NNP', loc='')\n",
    "    Featurize(w='have', POS='VBP', loc='b')\n",
    "    Featurize(w='Boris', POS='NNP', loc='a')\n",
    "\n",
    "Return the following corresponding dictionaries:\n",
    "\n",
    "    {'D': 1, 'C': 3, 'W3': '7', 'W2': '7', 'W1': '7', 'POS': 'NNP', 'POS2': 'NN'}\n",
    "    {'D': 0, 'C': 1, 'W3': 'AOL', 'W2': 'OL', 'W1': 'L', 'POS': 'NNP', 'POS2': 'NN'}\n",
    "    {'bD': 0, 'bC': 0, 'bW3': 'ave', 'bW2': 've', 'bW1': 'e', 'bPOS': 'VBP', 'bPOS2': 'VB'}\n",
    "    {'aD': 0, 'aC': 2, 'aW3': 'ris', 'aW2': 'is', 'aW1': 's', 'aPOS': 'NNP', 'aPOS2': 'NN'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9c0c61150eae08acac265f63b7d9e55",
     "grade": false,
     "grade_id": "Featurize_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def Featurize(w='AOL', POS='NNP', loc='') -> dict():\n",
    "    ''' Build a dictionary of features from the word, its POS and its location.\n",
    "            Features:\n",
    "                  D: 1 if w is a digit, 0 otherwise\n",
    "                  C: case feature, a value from Case2Num()\n",
    "                 W3: last 3 letters of w\n",
    "                 W2: last 2 letters of w\n",
    "                 W1: last letter of w\n",
    "                POS: POS\n",
    "               POS2: first 2 letters of POS\n",
    "               All keys are prepended with loc.\n",
    "        Inputs:\n",
    "          w: word being featurized. If not a string, convert to string.\n",
    "        loc: location of w yielding features(relative to the central (of concern) word in a window of 3 words)\n",
    "            'b'=before, 'a'=after, ''=current  \n",
    "        Returns: a dictionary of feature key-value pairs '''\n",
    "    DsFeatures = {'feature_name':'feature_value'}  # desired format of the output\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return DsFeatures\n",
    "\n",
    "print(Featurize(w='7', POS='NNP', loc=''))     # function demo\n",
    "print(Featurize(w='AOL', POS='NNP', loc=''))\n",
    "print(Featurize(w='have', POS='VBP', loc='b'))\n",
    "print(Featurize(w='Boris', POS='NNP', loc='a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64ac9cc3934ba501e715ed09bb9a879a",
     "grade": true,
     "grade_id": "Featurize_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class Test_Featurize(unittest.TestCase):\n",
    "    def test00(self): eq(type(Featurize()), dict)\n",
    "    def test01(self): eq(Featurize(w='7', POS='NNP', loc=''), {'C':3,'D': 1, 'POS': 'NNP', 'POS2': 'NN', 'W1': '7', 'W2': '7', 'W3': '7'})  \n",
    "    def test02(self): eq(Featurize(w='AOL', POS='NNP', loc=''), {'D':0,'C': 1, 'W3': 'AOL', 'W2': 'OL', 'W1': 'L', 'POS': 'NNP', 'POS2': 'NN'})  \n",
    "    def test03(self): eq(Featurize(w='have', POS='VBP', loc='b'), {'bD':0,'bC': 0, 'bW3': 'ave', 'bW2': 've', 'bW1': 'e', 'bPOS': 'VBP', 'bPOS2': 'VB'}) \n",
    "    def test04(self): eq(Featurize(w='Boris', POS='NNP', loc='a'), {'aD':0,'aC': 2, 'aW3': 'ris', 'aW2': 'is', 'aW1': 's', 'aPOS': 'NNP', 'aPOS2': 'NN'}) \n",
    "    def test05(self): eq(Featurize(1), {'C': 3, 'D': 1, 'POS': 'NNP', 'POS2': 'NN', 'W1': '1', 'W2': '1', 'W3': '1'})\n",
    "    def test06(self): eq(Featurize(w='iMac', POS='NNP', loc='a'), {'aD': 0, 'aC': 3, 'aW3': 'Mac', 'aW2': 'ac', 'aW1': 'c', 'aPOS': 'NNP', 'aPOS2': 'NN'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Features on a Text Window\n",
    " \n",
    "The UDF `WordWindow2Features()` applies the `Featurize()` function to each word on a window sliding along the text. Thus, each window generates a larger dictionary where features are labeled with the relation to the position of the word in a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordWindow2Features(WPE=LTsWPE3, i=0) -> dict():\n",
    "    '''Extract features from the central word, word before \n",
    "        and word after (unless central word starts/ends a sentence)\n",
    "    Inputs:\n",
    "        LTsWPE:   list of tuples of strings [(word, POS_Tag, NE_Tag), ...]. \n",
    "                    These tuples represent sequential words in a sentence.\n",
    "        i:        integer index of the central word. \n",
    "        Returns:  dictionary of features for the central word.      '''\n",
    "    Ft = Featurize(WPE[i][0], WPE[i][1]) # extract features from the center word and its POS tag\n",
    "    Ft.update({'b':1.})  # add a bias parameter to raise model flexibility\n",
    "    Ft.update(Featurize( WPE[i-1][0], WPE[i-1][1], loc='b') if i>0 else {'BOS':1}) # a word before center word\n",
    "    Ft.update(Featurize( WPE[i+1][0], WPE[i+1][1], loc='a') if i<(len(WPE)-1) else {'EOS':1}) # a word after center word\n",
    "    return Ft  # return dictionary of string keys with feature values (heterogeneous types)\n",
    "\n",
    "print(WordWindow2Features(i=0))  # Featurize 1st word in \"Yahoo fell today\"\n",
    "print(WordWindow2Features(i=1))  # Featurize 2nd word in \"Yahoo fell today\"\n",
    "print(WordWindow2Features(i=2))  # Featurize last word in \"Yahoo fell today\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `WordWindow2Features()` is rolled over the sequence of words to generate a set of features for each word based on itself and its neighboring words. This set of features is generated for each sentence separately using the `WPE2X()` function. The output labels (i.e., NE tags) are retrieved from the WPE triples using `WPE2Y()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WPE2X = lambda WPE: [WordWindow2Features(WPE, i) for i in range(len(WPE))]\n",
    "WPE2Y = lambda WPE: [NE for Word, POS, NE in WPE]  # pull out labels for each word in a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the features are generated by applying `WPE2X()` and `WPE2Y()` to each sentence in the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = [WPE2X(s) for s in tWPE] # training input features, list of list of dictionaries\n",
    "tY = [WPE2Y(s) for s in tWPE] # training outputs\n",
    "vX = [WPE2X(s) for s in vWPE] # test input features\n",
    "vY = [WPE2Y(s) for s in vWPE] # test outputs\n",
    "print('tY (train NE labels):', tY[0])\n",
    "print('tX (train features): ', str(tX[0])[:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CRF Model\n",
    "\n",
    "Next, the CRF model is trained on the training inputs and outputs. The hyperparameters can be experimented with to find a set of tuning parameters yielding improved performance of metrics on all or some desired NE tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = CRF.CRF(max_iterations=10, algorithm='lbfgs', c1=0.1, c2=0.1, all_possible_transitions=True, verbose=0)\n",
    "%time crf.fit(tX, tY)  # fit the model on training inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict NE Tags\n",
    " \n",
    "The trained `crf` model can now be used in predicting NE tags on test sentences featurized in the same way as the training sentences were. You can compare the actual and predicted NE tags. Notably, they are unlikely to match perfectly, but this leaves room for improvement of the model (via a better suite of hyperparameters or through a larger training set or through \"better\" features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = crf.predict(vX)                # predicted NE labels\n",
    "print('Actual NER tags:', vY[0])    # test (or validation) NE labels\n",
    "print('Predicted  tags:', pY[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Model Performance\n",
    " \n",
    "The model performance can be measured and printed with the `flat_classification_report()` function, which reports precision, recall and f-1 score for each NE tag. The support is the number of observations in each NE category. From here on, the scientist would evaluate what is most important for the business in this model and try to tune features and/or model to improve the model's prediction of those specific NE tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('O')    # suppress statistics for \"O\" tags\n",
    "pd.DataFrame(rpt(vY, pY, labels=labels, output_dict=True, zero_division=0)).round(2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
