{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup**\n",
    "\n",
    "In this notebook, you will be using the NLTK's [`TextBlob`](https://textblob.readthedocs.io/en/dev/) library, which contains many common and unique NLP functions, including a sentiment analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import pandas as pd, nltk\n",
    "from textblob import TextBlob  # version 0.17.1\n",
    "from sklearn.metrics import classification_report as rpt\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import movie_reviews\n",
    "_ = nltk.download(['movie_reviews', 'punkt', 'vader_lexicon'], quiet=True)\n",
    "pd.set_option('max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">\n",
    "\n",
    "# **Review**\n",
    "\n",
    "<font color='black'>[TextBlob](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis) is another library that you can use to easily evaluate the sentiment of a text. Similar to [SpaCy](https://spacy.io/), TextBlob wraps your text document into an object, analyzes it, and makes numerous [attributes](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis) and statistics accessible. In particular, it provides a sentiment polarity (as a number in $[-1,1]$ range) and sentiment subjectivity (as a number in $[0,1]$). A polarity of zero implies neutral sentiment. A subjectivity of zero indicates highly objective text.\n",
    "\n",
    "<font color='black'>Thus, the \"good idea\" text has a polarity of 0.7, indicating a strong positive opinion. Its subjectivity of 0.6 indicates an above average degree of opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TextBlob('good idea')\n",
    "print(tb.sentiment.polarity)                # in [-1, 1] interval\n",
    "print(round(tb.sentiment.subjectivity, 2))  # in [ 0, 1] interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lots of Good Ideas\n",
    "\n",
    "<font color='black'>As with VADER, you can measure evaluate different variants of the phrase \"good idea\" to learn what TextBlob's sentiment analysis algorithm is sensitive to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LsDocs = \\\n",
    "  ['Yes', 'No', 'Yes :-(', \"good idea\", \"GOOD idea\", \"good idea!\", \"good idea!!!\",\n",
    "   \"idea's good!!!!!!!!\", \"idea's good !!!!!!!!\", \"good idea!!!!!!!!\",      # too many exlamations may fail\n",
    "   \"not a good idea\", \"it isn't a good idea\", \"good and risky idea\",   # negation and multi-attitude towards the movie\n",
    "   \"idea is good, but risky\"]           # conjunction \"but\" sigmals change in polarity towards dominanty phrase\n",
    "\n",
    "def PolSub(sDoc='great idea!'):\n",
    "    tb = TextBlob(sDoc)\n",
    "    return (sDoc, tb.polarity, tb.subjectivity)\n",
    "\n",
    "df = pd.DataFrame([PolSub(s) for s in LsDocs], columns=['doc','pol','subj']).set_index('doc')\n",
    "df.T.style.background_gradient(cmap='coolwarm', vmin=-1, vmax=1).set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black'>Note that exclamations intensify the sentiment, but capitalization does not. Some negations and emoticons also impact the sentiment; however \"but\" and \"isn't\" do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob vs VADER on movie reviews dataset\n",
    "\n",
    "<font color='black'>Next, you'll compare TextBlob and VADER's performances on a much larger dataset. Start by loading NLTK's movie reviews, which include 1000 positive reviews and 1000 negative reviews. The cell below loads only 100 reviews from each category for performance reasons, but you can increase this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Categories:', movie_reviews.categories())\n",
    "print('Total Pos#: ', len(movie_reviews.fileids('pos')), ', Neg#:', len(movie_reviews.fileids('neg')))\n",
    "LsPos = [movie_reviews.raw(s) for s in movie_reviews.fileids('pos')[:100]] # retrieve a few positive reviews from files\n",
    "LsNeg = [movie_reviews.raw(s) for s in movie_reviews.fileids('neg')[:100]]\n",
    "LsReviews = LsPos + LsNeg     # concatenate lists of positive and negative reviews\n",
    "LnPosNeg = [1] * len(LsPos) + [-1] * len(LsNeg)   # actual (binary) polarities in {-1,1} set\n",
    "n = len(LsReviews)            # total count of reviews we selected\n",
    "print(LsReviews[:1][0][:265], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "<font color='black'>Since TextBlob's sentiment analyzer was pre-trained on movie reviews, it should perform well on this similar corpus. VADER, by contrast, is rule-based system curated by experts. You can tune it by altering the valence of words in its vocabulary or adding/deleting words.\n",
    "    \n",
    "<font color='black'>Next, you will train your own classifier on the corpus from your domain using these popular models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import classifiers\n",
    "print(', '.join(c for c in dir(classifiers) if 'Classifier' in c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "<font color='black'>Next, apply both models to the balanced subsamples. The dataframe generated below shows a small sample of TextBlob's results, including the actual sentiment polarity `vY` and the predicted valence `pPol`, which is thresholded at 0 to produce bi-polarity column `pY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pPol = [TextBlob(s).polarity for s in LsReviews]    # predicted polarities in [-1,1] interval\n",
    "\n",
    "pY = [-1 if p<0 else 1 for p in pPol]                     # predicted polarities in {-1,1} set\n",
    "dfTB = pd.DataFrame(dict(vY=LnPosNeg, pPol=pPol, pY=pY))  # Actual bi-polarity label, predicted polarity score, predicted bi-polarity label\n",
    "LnIX = list(range(20)) + list(range(n-20, n))             # index of top (pos) few and bottom (neg) few reviews\n",
    "dfTB.iloc[LnIX,:].T.style.background_gradient(cmap='coolwarm', vmin=-1, vmax=1).set_precision(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black'>Notice that TextBlob appears overly positive, misclassifying far more negative reviews than positive reviews. \n",
    "    \n",
    "<font color='black'>The next dataframe shows classification results for the VADER model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "%time pPol = [sia.polarity_scores(s)['compound'] for s in LsReviews]    # predicted polarities in [-1,1] interval\n",
    "\n",
    "pY = [-1 if p<0 else 1 for p in pPol]                                   # predicted polarities in {-1,1} set\n",
    "dfV = pd.DataFrame(dict(vY=LnPosNeg, pPol=pPol, pY=pY)) # Actual bi-polarity label, predicted polarity score, predicted bi-polarity label\n",
    "dfV.iloc[LnIX,:].T.style.background_gradient(cmap='coolwarm', vmin=-1, vmax=1).set_precision(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black'>Notice that VADER is more balanced in its misclassifications. It seemingly outperforms TextBlob on negative reviews and underperforms on positive reviews. Could you improve classification performance by combining these two models into an ensemble model? If you are interested, that exercise might be rewarding to explore on your own.\n",
    "\n",
    "<font color='black'>Finally, compare more comprehensive classification reports for the two model's outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rpt(y_true=dfTB.vY, y_pred=dfTB.pY, labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rpt(y_true=dfV.vY, y_pred=dfV.pY, labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='black'> Sentiment Metrics\n",
    "* <font color='black'>TextBlob's polarities are close to zero, while Vader's compound polarity is closer to +/-1.\n",
    "\n",
    "### <font color='black'>Speed\n",
    "\n",
    "* <font color='black'>VADER is twice as slow, but performs much better than TextBlob on negative reviews and equally well on positive reviews. \n",
    "    \n",
    "\n",
    "### <font color='black'>Re-training\n",
    "\n",
    "* <font color='black'>TextBlob can be re-trained on additional features and datasets. It uses NLTK's `NaiveBayesAnalyzer` to associate key words with binary sentiment\n",
    "* <font color='black'>You can quickly expand VADER's vocabulary by a few words, but TextBlob requires re-training to expand its vocabulary, whether by one word or one million words. In such training a \"sufficient\" number of examples must be provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">\n",
    "\n",
    "# **Optional Practice**\n",
    "\n",
    "<font color='black'>Now you will practice comparing metrics for each model.\n",
    "    \n",
    "As you work through these tasks, check your answers by running your code in the *#check solution here* cell, to see if you’ve gotten the correct result. If you get stuck on a task, click the See **solution** drop-down to view the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Task 1\n",
    "\n",
    "Compute f1 metrics for each sentiment class with all movie review observations with VADER and TextBlob models. Note the runtime. Do you agree with the performance and runtime comparisons made above considering this larger sample?\n",
    "\n",
    "<b>Hint:</b> Simply reuse the code above and remove slicing on <code>movie_reviews.fileids()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#606366>\n",
    "    <details><summary><font color=#B31B1B>▶ </font>See <b>solution</b>.</summary>\n",
    "<pre class=\"ec\">\n",
    "LsPos = [movie_reviews.raw(s) for s in movie_reviews.fileids('pos')] # retrieve a few positive reviews from files\n",
    "LsNeg = [movie_reviews.raw(s) for s in movie_reviews.fileids('neg')]\n",
    "LsReviews = LsPos + LsNeg\n",
    "LnPosNeg = [1] * len(LsPos) + [-1] * len(LsNeg)   # actual (binary) polarities in {-1,1} set\n",
    "\n",
    "%time pPol = [TextBlob(s).polarity for s in LsReviews]    # predicted polarities in [-1,1] interval\n",
    "dfTB = pd.DataFrame(dict(vY=LnPosNeg, pPol=pPol, pY=[-1 if p<0 else 1 for p in pPol]))\n",
    "%time pPol = [sia.polarity_scores(s)['compound'] for s in LsReviews]    # predicted polarities in [-1,1] interval\n",
    "dfV = pd.DataFrame(dict(vY=LnPosNeg, pPol=pPol, pY=[-1 if p < 0 else 1 for p in pPol])) \n",
    "print(rpt(y_true=dfTB.vY, y_pred=dfTB.pY, labels=[-1,1]))\n",
    "print(rpt(y_true=dfV.vY, y_pred=dfV.pY, labels=[-1,1]))\n",
    "            </pre>Yes, the runtime and f1 scores are fairly similar with this larger sample as they were with the smaller sample of 200 reviews. However, VADER also appears to underperform on positive reviews with a similar f1 score.\n",
    "</details> \n",
    "</font>\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
