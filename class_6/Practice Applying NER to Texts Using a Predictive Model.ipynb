{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup**\n",
    "\n",
    "<font color='black'>In this notebook, you will be using the [`sklearn_crfsuite`](https://sklearn-crfsuite.readthedocs.io/en/latest/) package, which integrates with scikit-learn ([SKL](https://scikit-learn.org/stable/)) to build a predictive model for named entity recognition (NER) tags. This package improves time and memory efficiency, but was developed for SKL version < 0.24. For that reason, SKL is downgraded to 0.24 in this notebook to avoid runtime errors.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U scikit-learn==0.23.2 > Log # SKL compatible with sklearn_crfsuite==0.3.6\n",
    "!pip freeze | grep learn\n",
    "# !pip show scikit-learn sklearn_crfsuite  # more details about packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black'>Reset the Python environment to clear it of any previously loaded variables, functions, or libraries. Then, import the libraries needed to complete the code Professor Melnikov presented in the video. The [`warnings`](https://docs.python.org/3/library/warnings.html) package is needed to suppress [`FutureWarning`](https://docs.python.org/3/library/exceptions.html#FutureWarning) alerts rising from the lower version of SKL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from IPython.core.interactiveshell import InteractiveShell as IS\n",
    "IS.ast_node_interactivity = \"all\"    # allows multiple outputs from a cell\n",
    "import pandas as pd, re, nltk, numpy as np, sklearn_crfsuite as CRF, warnings\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn_crfsuite.metrics import flat_classification_report as rpt  # model's detailed metrics\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # suppress FutureWarning warnings\n",
    "%time _ = nltk.download(['inaugural'], quiet=True)  # corpora for POS tagging: 'punkt', 'averaged_perceptron_tagger'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">\n",
    "\n",
    "# **Review**\n",
    "\n",
    "## Loading NER Sample\n",
    "\n",
    "<font color='black'>In this notebook, you will use the Groningen Meaning Bank ([GMB](https://gmb.let.rug.nl/)) NER annotated dataset (ner_dataset.csv) to train a predictive model to recognize NE tags from sentence/word features, such as capitalization, part of speech (POS), neighboring word attributes, etc. \n",
    "    \n",
    "<font color='black'>In the following cell, load the dataset file, `ner_dataset.csv.gz` into dataframe `df`, where rows represent words and their attributes, such as sentence ID, POS and NE tag. In this notebook, your sample is limited to the first several thousand rows to prevent memory errors.\n",
    "    \n",
    "<font color='black'>This model uses the NE tag as an output label. The challenge is that the NE tag is associated with a phrase (i.e., a chunk), but chunking in sentences is an expensive process. The model below will learn to move from parsed words directly to NE tags skipping explicit chunking.\n",
    "\n",
    "<font color='black'>The `NA` values in `SentID` column are forward-filled and redundant words are removed. </font>\n",
    "\n",
    "<!-- # sFilePath = 'https://raw.githubusercontent.com/omelnikov/CIS57x/main/ner_dataset.csv.gz' -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ner_dataset.csv.gz', compression='gzip', encoding='ISO-8859-1')[:100000]\n",
    "df.columns = ['SentID','Word','POS','NE']  # new (shorter) column names\n",
    "df = df.fillna(method='ffill')   # sequentially propagate (forward-fill) sentence IDs to replace NaNs\n",
    "df.SentID = df.SentID.apply(lambda s: int(s.replace('Sentence: ', '')))  # remove redundant \"Sentence: \" terms\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black'>You can reproduce any given sentence by concatenating all its words in the `Word` column sequentially.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(df[df.SentID==1].Word.tolist()))  # we can recover any sentence in its string form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOB Notation\n",
    "\n",
    "<span style=\"color:black\">The GMB dataset uses eight NE tags: gpe (or geopolitical), geo(graphical), org(anization), per(son), tim(e), art(ifact), eve(nt), nat(ural phenomenon). It also uses `IOB` notation for tagging each NE. Each word in a sentence is assigned one tag to indicate what part of an NE tag the word is or whether it's not an NE:\n",
    "    \n",
    "1. `I`: Tag is **inside** the chunk, or phrase\n",
    "2. `O`: Tag is **outside** the chunk\n",
    "3. `B`: Tag is **beginning** the chunk\n",
    "\n",
    "<span style=\"color:black\">Run the cell below to visualize `IOB` tagging.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DssFG = {'I-':'lightgray', 'O-':'gainsboro', 'B-':'black'} # foreground color for tags\n",
    "DssBG = dict(geo='khaki', gpe='orange', nat='purple', tim='lightblue', \\\n",
    "             per='cornflowerblue', art='lightgreen', eve='goldenrod', org='cyan')\n",
    "\n",
    "DsHeaderStyle = {'selector': 'th', 'props': [ ('font-size', '5pt'), ('color', 'lightgray')]}\n",
    "Style = lambda x: f'color: {DssFG.get((x+\"-\")[:2], \"\")}; background-color: {DssBG.get(x[2:], \"\")}'\n",
    "\n",
    "PrettySent = lambda n=2: df[df.SentID == n ][['Word','NE']].T.\\\n",
    "  style.applymap(Style).set_table_styles([DsHeaderStyle]).\\\n",
    "  set_properties(**{'text-align': 'center'})\n",
    "\n",
    "for i in [8,15]: PrettySent(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:black'>Consider the displayed sentences, where each word has either a colored NE tag below it or an uncolored `O`. All contiguous words in an NE share the same color, but only the first tag in the phrase starts with `B`. The rest start with `I`. Thus, in the phrase \"*International\tAtomic\tEnergy\tAgency*\", \"*International*\" is tagged with `B` and the remaining words tagged with `I`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\">Next, print the NE tag distribution.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.NE.value_counts().to_frame().T        # named entity distribution\n",
    "print(f'Total named entities: {sum(df.NE.str.startswith(\"B\").values)}') # counts NE tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\">Note that `per` has about the same beginning and inside tags, suggesting that most names contain first and last names in this text. You can make a similar observation about the `org` tag. By contrast, the `geo` tags indicate that the text contains three times more one-word geographical names than multi-word geographical names.\n",
    " \n",
    "<span style=\"color:black\">Use this distribution to build an expectation about the quality of predictions of these tags. More frequent tags (relative to the diversity of your named entities) are likely to show better predictions. If a higher prediction quality is needed for some NE tag, you may add more diverse training examples, look for attributes that describe the related named entities, or focus on tuning your predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Features\n",
    " \n",
    "<font color=\"black\">While you could force the model to memorize each NE in the training set, this would consume more storage/memory and is unlikely to generalize to new examples. For that reason, you should focus on finding good features of named entities. The model below has a standard set of features, including letter capitalization and POS tags. Features are built for the word in question and its surrounding words.\n",
    " \n",
    "<font color=\"black\">After some experimentation, you might add or remove features to improve a model's performance.  Take care to evaluate poor predictions, then decide on features that might be helpful to improve them. For example, digits are likely to be associated with time, so you may want a feature that indicates whether a word is a digit. Title capitalization may be helpful in identifying proper names, so it may make sense to include a title case feature. If you notice that most organizations follow with a word `'Co.'` or `'Inc.'`, then you might add a feature flagging the presence of these words.\n",
    "\n",
    "<font color=\"black\">To create features for this text, you can add a field with a tuple of strings (word, POS tag, NER tag). That field is then exported as `LLTsWPE`, or list of lists of tuples of strings of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TsWPE'] = list(zip(df.Word, df.POS, df.NE)) # WPE = Word, Part (of speech), (named) Entity\n",
    "df.head(10).T\n",
    "\n",
    "LLTsWPE = df.groupby('SentID')['TsWPE'].apply(list).tolist()  # list of lists of WPE tuples\n",
    "print(LLTsWPE[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black'>You may also want your model to be able to consider a word's neighboring words. The following UDF, `Featurize()` takes a word, its POS, and its relative position in a window of three words. Then, the word's location (`b`=before or `a`=after) is prepended to the key names to distinguish the attributes. \n",
    "\n",
    "<font color='black'>Then, the UDF extracts the features and packages them in a dictionary. Notice that these features just reflect reasonable guesses as to what attributes might relate to the given set of NE tags. They form a baseline for measuring future improvements when trying other sets of features.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Featurize(sWord='AOL', sPOS='NNP', sLoc='') -> dict():\n",
    "    ''' Build features from the word and its POS. \n",
    "        sLoc: location of word yielding features (relative to word of concern): \n",
    "        'b'=before, 'a'=after, ''=current  '''\n",
    "    return {sLoc + 'D' : sWord.isdigit() * 1, # is the word a digit\n",
    "            sLoc + 'UC': sWord.isupper()*1,   # is it capitalized\n",
    "            sLoc + 'TC': sWord.istitle()*1,   # is it in title case\n",
    "            sLoc + 'T' : sPOS,                # the full POS\n",
    "            sLoc + 'T2': sPOS[:2]}            # the short POS\n",
    "\n",
    "print(Featurize(sWord='7', sPOS='NNP', sLoc=''))      # function demo. build NNP features from a word containing a number\n",
    "print(Featurize(sWord='AOL', sPOS='NNP', sLoc=''))    # function demo. build NNP features from a word containing a capitals\n",
    "print(Featurize(sWord='have', sPOS='VBP', sLoc='b'))  # function demo. build VBP features from a word containing a verb\n",
    "print(Featurize(sWord='Boris', sPOS='NNP', sLoc='a')) # function demo. build NNP features from a word containing a title-cased name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black'>The UDF `WordWindow2Features()` takes a triplet of three words (along with their POS & NE tag attributes) and builds a dictionary of features for each word using the `Featurize()` UDF. It also adds a bias feature, i.e. responsible for training a free parameter (not dependent on any feature), which in linear models captures the additive shift in the distributions fitted to data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordWindow2Features(\n",
    "    LTsWPE=[('Yahoo','NNP','B-org'), ('fell','VBD','O'), ('today','NN','B-tim')], i=0) -> dict():\n",
    "    '''Extract features from the central word, word before \n",
    "        and word after (unless central word starts/ends a sentence)\n",
    "    Inputs:\n",
    "        LTsWPE:   list of tuples of strings [(word, POS_Tag, NE_Tag), ...]. \n",
    "                    These tuples represent sequential words in a sentence.\n",
    "        i:        integer index of the central word. \n",
    "        Returns:  dictionary of features for the central word.      '''\n",
    "    DsFt = Featurize(LTsWPE[i][0], LTsWPE[i][1]) # extract features from the center word and its POS tag\n",
    "    DsFt.update({'b':1.})  # add a bias parameter to raise model flexibility\n",
    "    DsFt.update(Featurize( LTsWPE[i-1][0], LTsWPE[i-1][1], sLoc='b') if i>0 else {'BOS':1}) # a word before center word\n",
    "    DsFt.update(Featurize( LTsWPE[i+1][0], LTsWPE[i+1][1], sLoc='a') if i<(len(LTsWPE)-1) else {'EOS':1}) # a word after center word\n",
    "    return DsFt  # return dictionary of string keys with feature values (heterogeneous types)\n",
    "\n",
    "print(WordWindow2Features(i=0))  # Featurize 1st word in \"Yahoo fell today\"\n",
    "print(WordWindow2Features(i=1))  # Featurize 2nd word in \"Yahoo fell today\"\n",
    "print(WordWindow2Features(i=2))  # Featurize last word in \"Yahoo fell today\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black'>The following two UDFs encode a sentence (a list of words with their POS and NE tags) and prepare a series of labels for each word in a sentence, i.e., a list of tuples of strings.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sent2Features = lambda LTsWPE: [WordWindow2Features(LTsWPE, i) for i in range(len(LTsWPE))]\n",
    "Sent2Labels   = lambda LTsWPE: [NE for Word, POS, NE in LTsWPE]  # pull out labels for each word in a sentence\n",
    "\n",
    "print(' '.join(list(zip(*LLTsWPE[0]))[0])) \n",
    "print(LLTsWPE[0][5:7])                  # 5th and 6th WPE elements to be featurized\n",
    "print(Sent2Features(LLTsWPE[0][5:7]))   # a list of feature dictionaries for 5th and 6th WPE elements\n",
    "print(Sent2Labels(LLTsWPE[0][5:7]))     # a list of named entity (NE) labels for 5th and 6th WPE elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Observations into Train and Test Sets\n",
    "\n",
    "<font color='black'>Finally, create input features from each sentence with their corresponding labels. Then, split these into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [Sent2Features(s) for s in LLTsWPE]\n",
    "Y = [Sent2Labels(s) for s in LLTsWPE]\n",
    "tX, vX, tY, vY = train_test_split(X, Y, test_size=0.25, random_state=0)\n",
    "print('tY (train NE labels):', tY[0])\n",
    "print('tX (train features): ', str(tX[0])[:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model\n",
    "\n",
    "<font color='black'>Now, you are ready to train a **conditional random fields** ([CRF](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)) model just like you would train any other model in SKL. You can tune the parameters below to improve the model performance, although using the full given sample and increasing iterations can result in an even greater performance improvement.\n",
    "\n",
    "<font color='black'>The CRF model estimates the probability $\\mathbb P[Y|X]$, where $Y$ and $X$ are unobserved and observed nodes in an undirected graphic, respectively. This model is often used in NER and similar tasks because of its superior (compared to Hidden Markov Model (HMM), for example) ability to segment and label text. Notice that segmenting (or chunking herein) is performed by CRF itself and not apriori.\n",
    "\n",
    "<font color='black'>The tuning parameters available for the [`CRF()`](https://sklearn-crfsuite.readthedocs.io/en/latest/api.html#module-sklearn_crfsuite) object are listed below. A few are already modified in the following cell, but you are encouraged to explore the others.\n",
    "\n",
    "* <font color='black'>`max_iterations` is currently set to 10 to avoid time-consuming fitting. If the optimal number of iterations is much larger, fewer iterations will result in poorer model performance.\n",
    "* <font color='black'>`algorithm='lbfgs'` is a limited memory version of the popular Broyden–Fletcher–Goldfarb–Shanno (BFGS) iterative algorithm for solving unconstrained nonlinear optimization tasks. It is a variant of the gradient descent method.\n",
    "* <font color='black'>`c1` and `c2` are coefficients for lasso and ridge regularizations which control the magnitude of the learned parameters. Larger regularization coefficients (unequally) forces model parameters towards zero, thereby lowering their effect in the model. These can be used to control model stability (i.e., bias-variance tradeoff of the output performance).\n",
    "* <font color='black'>`all_possible_transitions=True` forces the model to generate transition features, which were not specified in advance. Thus, given $L$ features, the model will try to fit $L^2$ extended features (original plus transitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?CRF.CRF   # show help manual for the hyper-parameters\n",
    "crf = CRF.CRF(max_iterations=10, algorithm='lbfgs', \n",
    "                           c1=0.1, c2=0.1, all_possible_transitions=True, verbose=0)\n",
    "crf.fit(tX, tY)  # fit the model on training inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict NE Tags\n",
    "\n",
    "<font color='black'>Now you can use the trained model `crf` to make predictions for each word in a sentence using the test set `vX`. Notice that the predicted output is no different than the actual output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = crf.predict(vX)                # predicted NE labels\n",
    "print('Actual NER tags:', vY[0])    # test (or validation) NE labels\n",
    "print('Predicted  tags:', pY[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Report\n",
    " \n",
    "<font color='black'>You can derive a more complete performance report using the built-in `flat_classification_report()` function. Here, evaluate which labels are performing relatively poorly based on their precision/recall or f1 metrics. \n",
    " \n",
    "<font color='black'>To improve the performance of the specific NE tags (or all of them), you could:\n",
    " \n",
    "1. Add more diverse examples for the NE categories\n",
    "1. Evaluate examples in the underperofrming NE category and deduce new features that appear to describe the considered named entities\n",
    "1. Tune the hyperparameters in the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('O')    # suppress statistics for \"O\" tags\n",
    "pd.DataFrame(rpt(vY, pY, labels=labels, output_dict=True, zero_division=0)).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER on New Text\n",
    "\n",
    "<font color='black'>Consider the `sDoc` below. After some preprocessing and POS tagging, you can use the model above to predict NE tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sDoc = \"\"\"Three more countries have joined an \"international grand \n",
    "    committee\" of parliaments, adding to calls for Facebook’s boss, Mark \n",
    "    Zuckerberg, to give evidence on misinformation to the coalition. Brazil, \n",
    "    Latvia and Singapore bring the total to eight different parliaments across \n",
    "    the world, with plans to send representatives to London on 27 November \n",
    "    with the intention of hearing from Zuckerberg. Since the Cambridge \n",
    "    Analytica scandal broke, the Facebook chief has only appeared in front of \n",
    "    two legislatures: the American Senate and House of Representatives, and \n",
    "    the European parliament. Facebook has consistently rebuffed attempts from \n",
    "    others, including the UK and Canadian parliaments, to hear from Zuckerberg. \n",
    "    He added that an article in the New York Times on Thursday, in which the \n",
    "    paper alleged a pattern of behaviour from Facebook to \"delay, deny and \n",
    "    deflect\" negative news stories, \"raises further questions about how recent \n",
    "    data breaches were allegedly dealt with within Facebook.\" \"\"\"\n",
    "sDoc = re.sub(r'\\n', '', sDoc)\n",
    "LsWords = nltk.word_tokenize(sDoc)\n",
    "LTsWP = nltk.pos_tag(LsWords)\n",
    "print(LTsWP[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black'>Featurize the text, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDsFeatures = [Sent2Features(LTsWP)]\n",
    "print(LDsFeatures[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black'>Now, the trained model can take the features, i.e. the list of dictionaries, and produce NE tags for each word. Since you passed the whole document at once, the returned list contains a single list element, which you can extract with index 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LsDocLabels = crf.predict(LDsFeatures)[0] # use trained model to predict NER tags\n",
    "print(LsDocLabels[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black'>Next, generate a more complete output with words and their NE tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 400)\n",
    "LTsWE = [(Word, NE) for Word, NE in zip(LsWords, LsDocLabels)]\n",
    "pd.DataFrame(LTsWE).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black'>Note that there is still room for model improvement. In particular, `international grand committee` is not tagged, while `Facebook` is tagged as `geo`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black'>You can only evaluate the tagged entities. `Mark Zuckerberg` is correctly identified as a person, and `Brazil`, `Latvia`, `Singapore`, and `London` are correctly identified as geographical names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNE = pd.DataFrame(LTsWE, columns=['word', 'NER_tag']).query('NER_tag!=\"O\"').set_index('word')  # simple display fails to combine named entities\n",
    "dfNE.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black'>Next, combine words that belong to a single entity and leave only their beginning NE tag, for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNE['id'] = list(range(len(dfNE)))                         # assign integer IDs to each row, 0, 1, 2, 3, ..., len(dfNE)\n",
    "dfNE.loc[dfNE.NER_tag.str.startswith('I-'), 'id'] = np.NaN  # replace id's with NaN wherever NER_tag starts with 'I-'\n",
    "dfNE.loc[dfNE.NER_tag.str.startswith('I-'), 'NER_tag'] = '' # replace NER_tag's with '' wherever NER_tag starts with 'I-'\n",
    "\n",
    "# forward-fill column gaps with a previous values of the column\n",
    "# Eg. column values [a,b,,,g] are changed to [a,b,b,b,g]\n",
    "dfNE = dfNE.fillna(method='ffill')                          \n",
    "\n",
    "# concatenate all values that have matching id's. Set resulting phrases as an index named 'word'\n",
    "dfNE = dfNE.reset_index().groupby('id').transform(lambda x: ' '.join(x)).drop_duplicates().set_index('word')\n",
    "dfNE.T        # print transposed dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">\n",
    "\n",
    "# **Optional Practice**\n",
    "\n",
    "<font color='black'>Now you will practice identifying NE tags in a new document using the model you pretrained above.\n",
    "    \n",
    "As you work through these tasks, check your answers by running your code in the *#check solution here* cell, to see if you’ve gotten the correct result. If you get stuck on a task, click the See **solution** drop-down to view the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Use NLTK's inaugural speeches to retrieve the words of President Biden's 2021 speech. Assign POS tags to these words and save these as an `LTsWP1` list of tuples of strings of words and POS tags (similar to `LTsWP` above).\n",
    "\n",
    "<b>Hint:</b> You can use <code>nltk.corpus.inaugural.fileids()</code> to list all inaugural speech file IDs. Then use <code>words()</code> method of <code>nltk.corpus.inaugural</code> object to retrieve the list of words of the speech. Use <code>nltk.pos_tag()</code> method to retrieve POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#606366>\n",
    "    <details><summary><font color=#B31B1B>▶ </font>See <b>solution</b>.</summary>\n",
    "<pre class=\"ec\">\n",
    "[s for s in nltk.corpus.inaugural.fileids() if 'Biden' in s]\n",
    "LTsWP1 = nltk.pos_tag(nltk.corpus.inaugural.words('2021-Biden.txt'))\n",
    "print(LTsWP1[:10])\n",
    "</pre>\n",
    "</details> \n",
    "</font>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Use `Sent2Features()` UDF to create `LDsFeatures1`, a dictionary of features from `LTsWP1`.\n",
    "\n",
    "<b>Hint:</b> This is similar to the application of <code>Sent2Features()</code> above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#606366>\n",
    "    <details><summary><font color=#B31B1B>▶ </font>See <b>solution</b>.</summary>\n",
    "<pre class=\"ec\">\n",
    "LDsFeatures1 = Sent2Features(LTsWP1)\n",
    "print(LDsFeatures1[0])\n",
    "</pre>\n",
    "</details> \n",
    "</font>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Use the trained `crf` model to predict a label for each word in the inaugural speech based on `LDsFeatures1` input to the trained model.\n",
    "\n",
    "<b>Hint:</b> This is similar to the application of the <code>crf.predict()</code> above. Note that <code>predict</code> expects a list of lists, so you may need to wrap `LDsFeatures1` into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#606366>\n",
    "    <details><summary><font color=#B31B1B>▶ </font>See <b>solution</b>.</summary>\n",
    "<pre class=\"ec\">\n",
    "LsDocLabels1 = crf.predict([LDsFeatures1])[0]   # use trained model to predict NER tags[0]\n",
    "print(LsDocLabels1[10:20])\n",
    "\n",
    "df1 = pd.DataFrame(LTsWP1)\n",
    "df1['NE'] = LsDocLabels1\n",
    "df1.T\n",
    "</pre>\n",
    "</details> \n",
    "</font>\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
